{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "def new_line():\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "def RMSE(predictions):\n",
        "    return round(np.sqrt(((test_y - predictions)**2).mean()))\n",
        "\n",
        "def plot_numerical_columns(col_name):\n",
        "    return None\n",
        "    df[col_name].plot(figsize=(13,8));\n",
        "    plt.title(col_name, size=18);\n",
        "    plt.axhline(y=df[col_name].mean(), color='red');\n",
        "    plt.axhline(y=df[col_name].median(), color='green');\n",
        "    plt.legend(['Actual', 'Mean', 'Median']);\n",
        "    plt.show()\n",
        "\n",
        "    df[col_name].sort_values().reset_index(drop=True).plot(figsize=(13,8));\n",
        "    plt.title(col_name+\" (SORTED)\", size=18);\n",
        "    plt.axhline(y=df[col_name].mean(), color='red');\n",
        "    plt.axhline(y=df[col_name].median(), color='green');\n",
        "    plt.legend(['Actual', 'Mean', 'Median']);\n",
        "    plt.show()\n",
        "\n",
        "    df[col_name].plot(kind=\"box\", figsize=(13,8))\n",
        "    plt.title(col_name, size=18);\n",
        "    plt.xlabel(\"\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_date_columns(col_name):\n",
        "    return None\n",
        "    df[col_name].plot(figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Index\", size=14);\n",
        "    plt.ylabel(\"Date\", size=14);\n",
        "    plt.title(col_name + \" Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    df[col_name].sort_values().reset_index(drop=True).plot(figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Index (sorted)\", size=14);\n",
        "    plt.ylabel(\"Year\", size=14);\n",
        "    plt.title(col_name + \" Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.year.value_counts(sort=False).sort_index() / len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Year\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" year Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.month.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Month\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" month Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.day.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Day\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" Day Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "def plot_catagorical_columns(cat_variable):\n",
        "    return None\n",
        "    (df[cat_variable].value_counts() / len(df) * 100).plot.bar(figsize=(15,6), grid=True);\n",
        "    plt.title(cat_variable, size=18, color='r');\n",
        "    plt.xlabel(\"Catagory\", size=14, color='r');\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14, color='r');\n",
        "    plt.show()\n",
        "\n",
        "def data_shape():\n",
        "    return f\"The Data have:\\n\\t{df.shape[0]} rows\\n\\t{df.shape[1]} columns\\n\"\n",
        "#===\n",
        "# df = pd.read_csv(\"data.csv\", date_parser=True)\n",
        "\n",
        "df = pd.read_csv(\"df_only_selected_columns_using_PCA.csv\", date_parser=True)\n",
        "target_variable = \"ACTUAL_WORTH\"\n",
        "\n",
        "# df = pd.read_csv(\"cleaned_data.csv\", date_parser=True)\n",
        "# target_variable = \"SalePrice\"\n",
        "#===\n",
        "f = df[target_variable].isna().sum()\n",
        "if f:\n",
        "    new_line()\n",
        "    print(f\"There are {f} NAs in target values, we droped those rows\")\n",
        "    df = df[df[target_variable].notna()]\n",
        "del f\n",
        "#---------------------------------------------------\n",
        "# df.select_dtypes(\"O\").columns[:5]\n",
        "# D = df.select_dtypes(exclude=\"O\")\n",
        "# D2 = df.select_dtypes(\"O\").iloc[:,:5]\n",
        "# df = pd.concat([D, D2], 1)\n",
        "\n",
        "# from pandas_profiling import ProfileReport\n",
        "# profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
        "# profile.to_file(\"your_report.html\")\n",
        "#---------------------------------------------------\n",
        "new_line()\n",
        "print(data_shape())\n",
        "#===\n",
        "new_line()\n",
        "print(f\"Columns types distribution:\\n\\n{df.dtypes.value_counts()}\")\n",
        "#---------------------------------------- NA\n",
        "a = df.isna().sum().where(lambda x:x>0).dropna()\n",
        "if a.size:\n",
        "    new_line()\n",
        "    print(f\"There are {len(a)} (out of {df.shape[1]}, [{round(len(a)/df.shape[1]*100)}%]) columns that contains 1 or more NA\")\n",
        "#===\n",
        "a = a.sort_values()/len(df)*100\n",
        "if (a == 100).sum():\n",
        "    new_line()\n",
        "    df.drop(columns=a[a==100].index, inplace=True)\n",
        "    print(f\"There are {(a == 100).sum()} columns that are all Missing values, so we droped those.\\nNow {data_shape()}\\n\\nDropped columns names:\")\n",
        "    for i in a[a==100].index:\n",
        "        print(\"\\t\",i)\n",
        "    a = a[a != 100]\n",
        "#===\n",
        "x = df[a.index].dtypes.value_counts()\n",
        "if x.size:\n",
        "    new_line()\n",
        "    print(f\"NA columns data type Distribution:\\n\\n{x}\")\n",
        "del x\n",
        "#===\n",
        "new_line()\n",
        "if a.size:\n",
        "    print(f\"NaN Ratio (0-100)\\n\\n{a}\")\n",
        "else:\n",
        "    print(\"Now There is no NaN value in our Data\")\n",
        "#===\n",
        "# IMPUTING missing values??????????????\n",
        "#===\n",
        "# --------------------------------------------------------- Unique values\n",
        "only_one_unique_value = df.nunique().where(lambda x:x == 1).dropna()\n",
        "if only_one_unique_value.size:\n",
        "    new_line()\n",
        "    df.drop(columns=only_one_unique_value.index, inplace=True)\n",
        "    print(f\"There are {only_one_unique_value.size} variables That have only one unique value, so we drop those.\\n\\nNow {data_shape()}\\n\\nThose columns names in order:\\n\")\n",
        "    for i in only_one_unique_value.index.sort_values():\n",
        "        print(i)\n",
        "del only_one_unique_value\n",
        "# #===\n",
        "all_values_are_unique = df.apply(lambda x:x.is_unique).where(lambda x:x==True).dropna()\n",
        "if all_values_are_unique.size:\n",
        "    new_line()\n",
        "    df.drop(columns=all_values_are_unique.index, inplace=True)\n",
        "    print(f\"There are {all_values_are_unique.size} column/s that have all unique values, so no value repeatation, we droped those columns.\\n\\nNow {data_shape()}\\nThose column/s name/s are:\\n\")\n",
        "    for i in all_values_are_unique.index:\n",
        "        print(\"\\t\", i)\n",
        "del all_values_are_unique\n",
        "#===\n",
        "date_columns = []\n",
        "def DTYPES():\n",
        "    global date_columns\n",
        "    catagorical_columns = df.head().select_dtypes(\"O\").columns\n",
        "    numerical_columns   = df.head().select_dtypes(\"number\").columns\n",
        "    date_columns        = []\n",
        "\n",
        "    for i in catagorical_columns:\n",
        "        try:\n",
        "            df[i] = pd.to_datetime(df[i])\n",
        "            date_columns.append(i)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    catagorical_columns = catagorical_columns.drop(date_columns)\n",
        "    if date_columns:\n",
        "        date_columns = pd.Index(date_columns)\n",
        "    #===\n",
        "    if not catagorical_columns.append(numerical_columns).append(date_columns).is_unique:\n",
        "        new_line()\n",
        "        print(\"\\nSome column/s repated in > 1 dtypes\\n\")\n",
        "        dtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "                    \"dtype\" : ['O']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "        print(dtypes[dtypes.Column.isin(list(dtypes[dtypes.Column.duplicated()].Column.values))].to_string())\n",
        "    #===\n",
        "    x = df.columns.difference(\n",
        "        catagorical_columns.append(numerical_columns).append(date_columns)\n",
        "        )\n",
        "    if x.size:\n",
        "        new_line()\n",
        "        print(\"Some columns not included in any existing catagory, those:\\n\")\n",
        "        for i in x:\n",
        "            print(f\"\\t<{i}, with dtype of <{df[i].dtype}>\")\n",
        "    #===\n",
        "    dtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "                \"dtype\" : ['Object']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "    return dtypes\n",
        "#===\n",
        "dtypes = DTYPES()\n",
        "# ----------------------------------------------------------------------- Feature enginearing\n",
        "# ======= Adding date columns\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> add polynomial, sqrt, tree, log features\n",
        "def add_new_date_cols(x, suffix):\n",
        "    d = {}\n",
        "    d[suffix + '_week_normalized'] = x.dt.week / 52\n",
        "    d[suffix + '_week_str'] = '\"' + x.dt.week.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_year_after_min_year'] = x.dt.year - x.dt.year.min()\n",
        "    d[suffix + '_year_str'] = '\"' + x.dt.year.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_day_name']  = x.dt.day_name()\n",
        "\n",
        "    d[suffix + '_day_after_min_date_str']  = '\"' + (x - x.min()).apply(lambda x: str(x).split()[0]) + '\"'\n",
        "\n",
        "    d[suffix + '_day_normalized'] = x.dt.day / 31\n",
        "\n",
        "    d[suffix + '_hour_normalized'] = x.dt.hour / 24\n",
        "    d[suffix + '_hour_str'] = '\"' + x.dt.hour.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_month_name'] = x.dt.month_name()\n",
        "    d[suffix + '_month_normalized'] = x.dt.month/12\n",
        "    for k,v in d.items():\n",
        "        if v.nunique() > 1:\n",
        "            df[k] = v\n",
        "    return df.drop(columns=x.name)\n",
        "    # return df\n",
        "\n",
        "len_df_before_adding_date_vars = df.shape[1]\n",
        "for date_col in date_columns:\n",
        "    df = add_new_date_cols(df[date_col], date_col)\n",
        "len_df_after_adding_date_vars  = df.shape[1]\n",
        "if len_df_after_adding_date_vars > len_df_before_adding_date_vars:\n",
        "    new_line()\n",
        "    print(f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} date Features\\n\")\n",
        "# ======= type casting of numerical variable (those who have < 4% unique values) to catagorical variables\n",
        "f = (df.select_dtypes(\"number\").nunique() / len(df) * 100).where(lambda x:x<4).dropna().index\n",
        "if f.size:\n",
        "    len_df_before_adding_date_vars = df.shape[1]\n",
        "    for col_num_to_str in f:\n",
        "        df[col_num_to_str+\"_str\"] = '\"' + df[col_num_to_str].astype(str) + '\"'\n",
        "    len_df_after_adding_date_vars  = df.shape[1]\n",
        "    new_line()\n",
        "    print(f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} String Features (Extracted from numerical variables)\\n\")\n",
        "# =======\n",
        "def cluping_rare_cases_in_one_catagory(x):\n",
        "    global df\n",
        "    x = df[x]\n",
        "    orignal  = x.copy(\"deep\")\n",
        "    xx = x.value_counts()\n",
        "    xx = xx[xx< 10].index.to_list()\n",
        "    x =  x.replace(xx , \"Rare cases\")\n",
        "    if x.value_counts()[-1] < 8:\n",
        "        x[x == \"Rare cases\"] = x.mode()[0] # agar \"Rare cases\" vali catogery me 8 sy bhi kam values hon to un ko most common value sy replace kar do\n",
        "    if x.nunique() == 1:\n",
        "        new_line()\n",
        "        print(f\"The column <{x.name}> have only one unique value, We droped it from the data.\")\n",
        "        # return orignal\n",
        "        df.drop(columns=x.name, inplace=True)\n",
        "        return None\n",
        "    return x\n",
        "\n",
        "for var in df.select_dtypes(\"O\").columns:\n",
        "    m = cluping_rare_cases_in_one_catagory(var)\n",
        "    if isinstance(m, pd.core.series.Series):\n",
        "        df[var] = m\n",
        "new_line()\n",
        "print(f\"<Rare case> catagory count:\\n{(df == 'Rare cases').sum().sort_values().where(lambda x:x>0).dropna().to_string()}\")\n",
        "# ----------------------------------------------------------------------- END (Feature enginearing)\n",
        "dtypes = DTYPES()\n",
        "# ---------------------------------------------------- Correlation plot\n",
        "cor_df = df.select_dtypes('number').corr().abs()\n",
        "mask = np.triu(np.ones_like(cor_df, dtype=bool));\n",
        "f, ax = plt.subplots(figsize=(17, 10));\n",
        "cmap = sns.color_palette(\"viridis\", as_cmap=True);\n",
        "plot_ = sns.heatmap(cor_df, mask=mask, cmap=cmap, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
        "plot_.axes.set_title(\"abs (Correlation) plot\",fontsize=25)\n",
        "# ---------------------------------------------------------------------\n",
        "#===\n",
        "# m = 0\n",
        "for row in dtypes.iterrows():\n",
        "    # m += 1\n",
        "    # if m == 3:\n",
        "        # break\n",
        "    column_name, type_ = row[1]\n",
        "    x = df[column_name]\n",
        "    print(f\"\\n\\n\\n=============================== {column_name} ===============================\\n\\n\")\n",
        "\n",
        "    for col_ in df.columns:\n",
        "        if col_ == column_name:\n",
        "            continue\n",
        "        if df[col_].nunique() == df[column_name].nunique():\n",
        "            unique_combination = df[[col_, column_name]].drop_duplicates()\n",
        "            if unique_combination.apply(lambda x:x.is_unique).sum() == 2:\n",
        "                new_line()\n",
        "                print(f\"This Columns is duplicate of <{col_}> column\")\n",
        "\n",
        "    print(f\"Column Type     : {type_}\")\n",
        "    if x.isna().all():\n",
        "        new_line()\n",
        "        df.drop(columns=column_name, inplace=True)\n",
        "        print(\"We dropped This column, because it is all Empty\")\n",
        "        continue\n",
        "    if type_ in [\"O\", \"Date\"]:\n",
        "        if x.is_unique:\n",
        "            new_line()\n",
        "            df.drop(columns=column_name, inplace=True)\n",
        "            print(f\"We dropped This column, because it's a {type_} columns, and it's all values are unique\")\n",
        "            continue\n",
        "    if x.nunique() == 1:\n",
        "        new_line()\n",
        "        df.drop(columns=column_name, inplace=True)\n",
        "        print(f\"We dropped This column, because There is only one unique value\")\n",
        "        continue\n",
        "\n",
        "    if type_ == \"Number\":\n",
        "        local_cor = cor_df[column_name].drop(column_name).reset_index()\n",
        "        local_cor = local_cor.reindex(local_cor[column_name].abs().sort_values().index)\n",
        "        if local_cor[column_name].max() == 1:\n",
        "            new_line()\n",
        "            print(f\"This column is perfactly correlated with column <{local_cor[local_cor[column_name] == m]['index'].values[0]}, so remove one of them\")\n",
        "        new_line()\n",
        "        print(f\"Most 3 correlated features with this columns:\\n{local_cor[-3:].rename(columns={'index' : 'Column name', column_name : 'Correlation'}).reset_index(drop=True)}\\n\")\n",
        "\n",
        "        new_line()\n",
        "        skewness = x.skew(skipna = True)\n",
        "        if abs(skewness) < 0.5:\n",
        "            print(f\"The data is fairly symmetrical {skewness}\")\n",
        "        elif abs(skewness) < 1:\n",
        "            print(f\"The data are moderately skewed {skewness}\")\n",
        "        else:\n",
        "            print(f\"The data are highly skewed {skewness}\\nNote: When skewness exceed |1| we called it highly skewed\")\n",
        "\n",
        "        # f = x.describe()\n",
        "        # f['Nunique'] = x.nunique()\n",
        "        # f['Nunique ratio'] = f.loc[\"Nunique\"] / f.loc[\"count\"] * 100\n",
        "        # f['Outlies count'] = (((x - x.mean())/x.std()).abs() > 3).sum()\n",
        "        # f['Outlies ratio'] = f.loc[\"Outlies count\"] / f.loc[\"count\"] * 100\n",
        "        # f['Nagative values count'] = (x < 0).sum()\n",
        "        # f['Nagative values ratio'] = f['Nagative values count'] / f['count'] * 100\n",
        "\n",
        "        ff = [x.count(), x.isna().sum(), x.mean(), x.std(), x.min()]\n",
        "        ff += x.quantile([.25,.5,.75]).to_list()\n",
        "        ff += [x.max(), x.nunique(), (((x - x.mean())/x.std()).abs() > 3).sum(), (x < 0).sum(), (x == 0).sum()]\n",
        "\n",
        "        f = pd.DataFrame(ff, index=['Count', 'NA', 'Mean', 'Std', 'Min', '25%', '50%', '75%', 'Max', 'Nunique', 'Outlies', 'Nagetive', 'Zeros'], columns=['Count'])\n",
        "        f['Ratio'] = f.Count / x.count() * 100\n",
        "        f.loc['Mean' : 'Max', 'Ratio'] = None\n",
        "\n",
        "        new_line()\n",
        "        print(f.round(2).to_string())\n",
        "        plot_numerical_columns(column_name)\n",
        "\n",
        "    elif type_ == \"Object\":\n",
        "        # f = x.describe()\n",
        "        # f = x.agg(['count', pd.Series.nunique])\n",
        "        # f['len'] = len(x)\n",
        "        # f['Na count'] = x.isna().sum()\n",
        "        # f['Na ratio'] = f['Na count'] / f['count'] * 100\n",
        "        # f['Most frequent'] = x.mode().values[0]\n",
        "        # f['Most frequent count'] = (x == f['Most frequent']).sum()\n",
        "        # f['Most frequent ratio'] = f['Most frequent count'] / f['count'] * 100\n",
        "        # f['Least frequent'] = x.value_counts().tail(1).index[0]\n",
        "        # f['Least frequent count'] = (x == f['Least frequent']).sum()\n",
        "        # f['Least frequent ratio'] = f['Least frequent count'] / f['count'] * 100\n",
        "        # f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        # f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "        l = x.count(), x.nunique(), len(x), x.isna().sum(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        f = pd.DataFrame(l, index=['Count', 'Nunique', 'Len', 'NA', 'Most frequent', 'Least frequent', 'Values occured only once'], columns=['Counts'])\n",
        "        f['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "        f.loc[['Count', 'Len'], 'Ratio'] = None\n",
        "\n",
        "        new_line()\n",
        "        print(f.to_string())\n",
        "\n",
        "\n",
        "        if x.str.lower().nunique() != x.nunique():\n",
        "            new_line()\n",
        "            print(f\"\\n\\nCase issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin lower verstion there are   {x.str.lower().nunique()} unique values.\")\n",
        "\n",
        "        if x.str.strip().nunique() != x.nunique():\n",
        "            new_line()\n",
        "            print(f\"\\n\\nSpace issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin striped verstion there are {x.str.strip().nunique()} unique values.\")\n",
        "\n",
        "        plot_catagorical_columns(column_name)\n",
        "\n",
        "    elif type == \"Date\":\n",
        "\n",
        "        new_line()\n",
        "        from dateutil import relativedelta\n",
        "        rd = relativedelta.relativedelta( pd.to_datetime(x.max()), pd.to_datetime(x.min()))\n",
        "        print(f\"Diffrenece between first and last date:\\n\\tYears : {rd.years}\\n\\tMonths: {rd.months}\\n\\tDays  : {rd.days}\\n\\n\")\n",
        "\n",
        "        # f = pd.Series({'Count' : x.count(),\n",
        "        #             'Nunique count' : x.nunique(),\n",
        "        #             'Nunique ratio' : x.nunique() / x.count() * 100,\n",
        "        #             'Most frequent value' : str(x.mode()[0]),\n",
        "        #             'Least frequent value' :  x.value_counts().tail(1).index[0]\n",
        "        #             })\n",
        "        # f['Most frequent count'] = (x == f['Most frequent value']).sum()\n",
        "        # f['Most frequent ratio'] = f['Most frequent count'] / f['Count'] * 100\n",
        "        # f['Least frequent count'] = (x == f['Least frequent value']).sum()\n",
        "        # f['Least frequent ratio'] = f['Least frequent count'] / f['Count'] * 100\n",
        "        # f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        # f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "        ff = x.count(), x.nunique(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        f = pd.DataFrame(ff, index=[\"Count\", 'Nunique', 'Most frequent values', 'Least frequent values', 'Values occured only once count'], columns=['Counts'])\n",
        "        f['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "\n",
        "        new_line()\n",
        "        print(f\"\\n{f.to_string()}\\n\\n\")\n",
        "\n",
        "\n",
        "        f = set(np.arange(x.dt.year.min(),x.dt.year.max()+1)).difference(\n",
        "            x.dt.year.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(f\"These Years (in order) are missing:\\n\")\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "        f = set(np.arange(x.dt.month.min(),x.dt.month.max()+1)).difference(\n",
        "            x.dt.month.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(f\"These Months (in order) are missing:\\n\")\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "        f = set(np.arange(x.dt.day.min(),x.dt.day.max()+1)).difference(\n",
        "            x.dt.day.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(f\"These Days (in order) are missing:\\n\")\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "        new_line()\n",
        "        plot_date_columns(column_name)\n",
        "\n",
        "\n",
        "df['BldgType'].value_counts()\n",
        "# ================================================================================================================ Modeling\n",
        "# --------------------------------------------------------- Linear regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_T = df.select_dtypes(\"number\")\n",
        "cat_cols = pd.get_dummies(df.select_dtypes(exclude=\"number\"), prefix_sep=\"__\")\n",
        "df_T[cat_cols.columns.to_list()] = cat_cols\n",
        "\n",
        "df = df_T.copy(\"deep\")\n",
        "del df_T\n",
        "del cat_cols\n",
        "# # ====\n",
        "train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "model_reg = OLS(train_y, train_X).fit()\n",
        "summary = model_reg.summary()\n",
        "summary_df = pd.DataFrame(summary.tables[1])\n",
        "summary_df.columns = summary_df.iloc[0]\n",
        "summary_df.drop(0, inplace=True)\n",
        "summary_df.columns = summary_df.columns.astype(str)\n",
        "summary_df.columns = [\"Variable\"] + summary_df.columns[1:].to_list()\n",
        "for i in summary_df.columns[1:]:\n",
        "    summary_df[i] = summary_df[i].astype(str).astype(float)\n",
        "summary_df.Variable = summary_df.Variable.astype(str)\n",
        "summary_df['Indicator'] = summary_df['P>|t|'].apply(lambda x:\"***\" if x < 0.001 else \"**\" if x < 0.01 else \"*\" if x < 0.05 else \".\" if x < 0.1  else \"\")\n",
        "summary_df = summary_df.sort_values(\"Variable\").reset_index(drop=True)\n",
        "# summary_df.to_csv()\n",
        "new_line()\n",
        "print(\"\\nNOTE: This summary saved as <summary_OLS_1.csv>\\n\\n\")\n",
        "# print(summary_df.to_string())\n",
        "# ============================= Model statistic\n",
        "predictions = model_reg.predict(test_X)\n",
        "\n",
        "new_line()\n",
        "print(\" --- Model statistic --- \\n\")\n",
        "print(f\"R-squared         : {round(model_reg.rsquared, 3)}\")\n",
        "print(f\"Adj. R-squared    : {round(model_reg.rsquared_adj, 3)}\")\n",
        "print(f\"F-statistic       : {round(model_reg.fvalue)})\")\n",
        "print(f\"Prob (F-statistic): {model_reg.f_pvalue}\")\n",
        "print(f\"No. Observations  : {round(model_reg.nobs)}\")\n",
        "print(f\"AIC               : {round(model_reg.aic)}\")\n",
        "print(f\"Df Residuals      : {round(model_reg.df_resid)}\")\n",
        "print(f\"BIC               : {round(model_reg.bic)}\")\n",
        "print(f\"RMSE (test)       : {RMSE(predictions)}\")\n",
        "print(\"\\n\")\n",
        "# ======\n",
        "f = train_X.copy(\"deep\")\n",
        "f['Errors__'] = model_reg.resid\n",
        "f = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "new_line()\n",
        "print(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "print(f\"Mean of train reseduals: {model_reg.resid.mean()}\")\n",
        "del f\n",
        "# ============================= END (Model statistic)\n",
        "# --------------------------------------------------------- END Linear regression\n",
        "# --------------------------------------------------------- Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators = 200, oob_score=True)\n",
        "model_rf = rf.fit(train_X, train_y);\n",
        "predictions_rf = rf.predict(test_X)\n",
        "\n",
        "new_line()\n",
        "print(f\"RF model peramters:\\n\\n\")\n",
        "import pprint\n",
        "pprint.pprint(m.get_params())\n",
        "\n",
        "new_line()\n",
        "importances = list(rf.feature_importances_)\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(test_X, importances)]\n",
        "featuresImportance = pd.Series(model_rf.feature_importances_, index=train_X.columns).sort_values(ascending=False)\n",
        "if len(featuresImportance) > 30:\n",
        "    featuresImportance = featuresImportance.head(30)\n",
        "featuresImportance.plot(figsize=(20,10), kind='bar', grid=True);\n",
        "plt.title(\"RandomForest Feature importances Graph\", size=18,color='red');\n",
        "plt.xlabel(\"Features\", size=14, color='red');\n",
        "plt.ylabel(\"Importance\", size=14, color='red');\n",
        "plt.show();\n",
        "del featuresImportance\n",
        "\n",
        "new_line()\n",
        "# The coefficient of determination R^2 of the prediction.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "print(f\"R^2 (test) : {rf.score(test_X, test_y)}\")\n",
        "print(f\"R^2 (train): {rf.score(train_X, train_y)}\")\n",
        "print(f\"RMSE (test): {RMSE(predictions_rf)}\")\n",
        "print(f\"oob score  : {model_rf.oob_score_}\")\n",
        "\n",
        "f = test_X.copy(\"deep\")\n",
        "errors_rf = predictions_rf - test_y\n",
        "f['Errors__'] = errors_rf\n",
        "f = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "new_line()\n",
        "print(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "# --------------------------------------------------------- END Random Forest\n",
        "\n",
        "# ================================================================================================================ END Modeling\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}