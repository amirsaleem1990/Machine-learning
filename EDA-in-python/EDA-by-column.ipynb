{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas_profiling import ProfileReport\n",
        "from dateutil import relativedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "from sklearn import metrics\n",
        "import missingno as msno\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pickle\n",
        "import pprint\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "modeling_   = False\n",
        "# modeling_   = True\n",
        "\n",
        "plot_______ = False\n",
        "# plot_______ = True\n",
        "\n",
        "summary__   = True\n",
        "# summary__   = False\n",
        "\n",
        "\n",
        "def add_new_date_cols(x, suffix):\n",
        "\td = {}\n",
        "\td[suffix + '_week_normalized'] = x.dt.week / 52\n",
        "\td[suffix + '_week_str'] = '\"' + x.dt.week.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "\td[suffix + '_year_after_min_year'] = x.dt.year - x.dt.year.min()\n",
        "\td[suffix + '_year_str'] = '\"' + x.dt.year.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "\td[suffix + '_day_name']  = x.dt.day_name()\n",
        "\n",
        "\td[suffix + '_day_after_min_date_str']  = '\"' + (x - x.min()).apply(lambda x: str(x).split()[0]) + '\"'\n",
        "\n",
        "\td[suffix + '_day_normalized'] = x.dt.day / 31\n",
        "\n",
        "\td[suffix + '_hour_normalized'] = x.dt.hour / 24\n",
        "\td[suffix + '_hour_str'] = '\"' + x.dt.hour.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "\td[suffix + '_month_name'] = x.dt.month_name()\n",
        "\td[suffix + '_month_normalized'] = x.dt.month/12\n",
        "\tfor k,v in d.items():\n",
        "\t\tif v.nunique() > 1:\n",
        "\t\t\tdf[k] = v\n",
        "\treturn df.drop(columns=x.name)\n",
        "\t# return\n",
        "\n",
        "def new_line():\n",
        "\tprint(\"\\n-------------------------\\n\")\n",
        "\n",
        "def RMSE(predictions):\n",
        "\treturn round(np.sqrt(((test_y - predictions)**2).mean()))\n",
        "\n",
        "def cluping_rare_cases_in_one_catagory(x):\n",
        "\tglobal df\n",
        "\tx = df[x]\n",
        "\torignal  = x.copy(\"deep\")\n",
        "\txx = x.value_counts()\n",
        "\txx = xx[xx< 10].index.to_list()\n",
        "\tx =  x.replace(xx , \"Rare cases\")\n",
        "\tif x.value_counts()[-1] < 8:\n",
        "\t\tx[x == \"Rare cases\"] = x.mode()[0] # agar \"Rare cases\" vali catogery me 8 sy bhi kam values hon to un ko most common value sy replace kar do\n",
        "\tif x.nunique() == 1:\n",
        "\t\tnew_line()\n",
        "\t\t# to_print = f\"The column <{x.name}> have only one unique value, We droped it from the data.\"\n",
        "\t\tto_print = f\"The column <{x.name}> have imbalanced, so we droped it, it has {orignal.nunique()} unique values, and most commont value frequency ratio is {(orignal == orignal.mode()[0]).mean()}\"\n",
        "\t\tprint(colored(to_print, 'red'))\n",
        "\t\t# return orignal\n",
        "\t\tdf.drop(columns=x.name, inplace=True)\n",
        "\t\treturn None\n",
        "\treturn x\n",
        "\n",
        "def plot_numerical_columns(col_name):\n",
        "\tif not plot_______:\n",
        "\t\treturn None\n",
        "\n",
        "\t# Histogram\n",
        "\tdf[col_name].plot(kind=\"hist\", figsize=(13,8));\n",
        "\tplt.title(col_name, size=18);\n",
        "\tplt.axhline(y=df[col_name].mean(), color='red');\n",
        "\tplt.axhline(y=df[col_name].median(), color='green');\n",
        "\tplt.legend(['Actual', 'Mean', 'Median']);\n",
        "\tplt.show()\n",
        "\n",
        "\t# Scatter plot\n",
        "\tdf[col_name].plot(figsize=(13,8));\n",
        "\tplt.title(col_name, size=18);\n",
        "\tplt.axhline(y=df[col_name].mean(), color='red');\n",
        "\tplt.axhline(y=df[col_name].median(), color='green');\n",
        "\tplt.legend(['Actual', 'Mean', 'Median']);\n",
        "\tplt.show()\n",
        "\n",
        "\t# scatter plot (sort by values), values Vs index\n",
        "\tdf[col_name].sort_values().reset_index(drop=True).plot(figsize=(13,8));\n",
        "\tplt.title(col_name+\" (SORTED)\", size=18);\n",
        "\tplt.axhline(y=df[col_name].mean(), color='red');\n",
        "\tplt.axhline(y=df[col_name].median(), color='green');\n",
        "\tplt.legend(['Actual', 'Mean', 'Median']);\n",
        "\tplt.show()\n",
        "\n",
        "\t# box plot\n",
        "\tdf[col_name].plot(kind=\"box\", figsize=(13,8))\n",
        "\tplt.title(col_name, size=18);\n",
        "\tplt.xlabel(\"\");\n",
        "\tplt.show()\n",
        "\n",
        "def plot_date_columns(col_name):\n",
        "\tif not plot_______:\n",
        "\t\treturn None\n",
        "\tdf[col_name].plot(figsize=(15,7), grid=True);\n",
        "\tplt.xlabel(\"Index\", size=14);\n",
        "\tplt.ylabel(\"Date\", size=14);\n",
        "\tplt.title(col_name + \" Graph\", size=18);\n",
        "\tplt.show();\n",
        "\n",
        "\tdf[col_name].sort_values().reset_index(drop=True).plot(figsize=(15,7), grid=True);\n",
        "\tplt.xlabel(\"Index (sorted)\", size=14);\n",
        "\tplt.ylabel(\"Year\", size=14);\n",
        "\tplt.title(col_name + \" Graph\", size=18);\n",
        "\tplt.show();\n",
        "\n",
        "\t(df[col_name].dt.year.value_counts(sort=False).sort_index() / len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "\tplt.xlabel(\"Year\", size=14);\n",
        "\tplt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "\tplt.title(col_name + \" year Frequency Graph\", size=18);\n",
        "\tplt.show();\n",
        "\n",
        "\t(df[col_name].dt.month.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "\tplt.xlabel(\"Month\", size=14);\n",
        "\tplt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "\tplt.title(col_name + \" month Frequency Graph\", size=18);\n",
        "\tplt.show();\n",
        "\n",
        "\t(df[col_name].dt.day.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "\tplt.xlabel(\"Day\", size=14);\n",
        "\tplt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "\tplt.title(col_name + \" Day Frequency Graph\", size=18);\n",
        "\tplt.show();\n",
        "\n",
        "def plot_catagorical_columns(cat_variable):\n",
        "\tif not plot_______:\n",
        "\t\treturn None\n",
        "\t(df[cat_variable].value_counts() / len(df) * 100).plot.bar(figsize=(15,6), grid=True);\n",
        "\tplt.title(cat_variable, size=18, color='r');\n",
        "\tplt.xlabel(\"Catagory\", size=14, color='r');\n",
        "\tplt.ylabel(\"Ratio (1-100)\", size=14, color='r');\n",
        "\tplt.show()\n",
        "\n",
        "def data_shape():\n",
        "\treturn f\"The Data have:\\n\\t{df.shape[0]} rows\\n\\t{df.shape[1]} columns\\n\"\n",
        "#===\n",
        "# df = pd.read_csv(\"data.csv\", date_parser=True)\n",
        "\n",
        "# df = pd.read_csv(\"df_only_selected_columns_using_PCA.csv\", date_parser=True)\n",
        "# target_variable = \"ACTUAL_WORTH\"\n",
        "# df = pd.concat([\n",
        "#         df.select_dtypes(\"number\").iloc[:, :3],\n",
        "#         df.select_dtypes(\"O\").iloc[:, :3],\n",
        "#         df.select_dtypes(exclude=[\"number\", \"O\"]),\n",
        "#         df[[target_variable]]], 1)\n",
        "# target_variable = \"AREA_NAME_EN\"\n",
        "\n",
        "# df = pd.read_csv(\"cleaned_data.csv\", date_parser=True)\n",
        "# target_variable = \"SalePrice\"\n",
        "\n",
        "train = pd.read_csv(\"/home/amir/Downloads/train.csv\")\n",
        "test  = pd.read_csv(\"/home/amir/Downloads/test.csv\")\n",
        "target_variable = \"SalePrice\"\n",
        "train_y = train[target_variable]\n",
        "train = train.drop(columns=target_variable)\n",
        "df = pd.concat([train, test])\n",
        "df[target_variable] = train_y.to_list() + [None]*len(test)\n",
        "#===\n",
        "new_line()\n",
        "print(data_shape())\n",
        "#===\n",
        "new_line()\n",
        "print(f\"Columns types distribution:\\n\\n{df.dtypes.value_counts()}\\n\")\n",
        "df.dtypes.value_counts().plot(kind='barh', figsize=(10, 2), grid=True, title=\"Variable types Count Graph\");\n",
        "plt.xlabel(\"Count\");\n",
        "plt.show()\n",
        "#===\n",
        "f = df[target_variable].isna().sum()\n",
        "if f:\n",
        "\tnew_line()\n",
        "\tto_print = f\"There are {f} NAs in target values, we droped those rows\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\tdf = df[df[target_variable].notna()]\n",
        "del f\n",
        "#---------------------------------------------------\n",
        "# df.select_dtypes(\"O\").columns[:5]\n",
        "# D = df.select_dtypes(exclude=\"O\")\n",
        "# D2 = df.select_dtypes(\"O\").iloc[:,:5]\n",
        "# df = pd.concat([D, D2], 1)\n",
        "\n",
        "# profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
        "# profile.to_file(\"your_report.html\")\n",
        "#---------------------------------------- NA\n",
        "a = df.isna().sum().where(lambda x:x>0).dropna()\n",
        "if a.size:\n",
        "\tnew_line()\n",
        "\tto_print = f\"There are {len(a)} (out of {df.shape[1]}, [{round(len(a)/df.shape[1]*100)}%]) columns that contains 1 or more NA.\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\tfor i in a.index:\n",
        "\t\tdf[i+\"_NA_indicator\"] = df[i].isna().replace({True : \"Missing\", False : \"Not missing\"})\n",
        "\tnew_line()\n",
        "\tto_print = f\"{a.size} NA_indicator variables added to the data\\n\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\n",
        "\tprint(\"========= NA Graphs =========\\n\")\n",
        "\tmsno.matrix(df);\n",
        "\tplt.title(\"NA Graph\");\n",
        "\tplt.show()\n",
        "\n",
        "\tnew_line()\n",
        "\tsns.heatmap(df.isnull(), cbar=False);\n",
        "\tplt.title(\"NA Graph\");\n",
        "\tplt.show()\n",
        "#===\n",
        "a = a.sort_values()/len(df)*100\n",
        "if (a == 100).sum():\n",
        "\tnew_line()\n",
        "\tdf.drop(columns=a[a==100].index, inplace=True)\n",
        "\tto_print = f\"There are {(a == 100).sum()} columns that are all Missing values, so we droped those.\\nNow {data_shape()}\\n\\nDropped columns names:\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\tfor i in a[a==100].index:\n",
        "\t\tprint(\"\\t\",i)\n",
        "\ta = a[a != 100]\n",
        "#===\n",
        "x = df[a.index].dtypes.value_counts()\n",
        "if x.size:\n",
        "\tnew_line()\n",
        "\tprint(f\"NA columns data type Distribution:\\n\\n{x}\")\n",
        "del x\n",
        "#===\n",
        "new_line()\n",
        "if a.size:\n",
        "\tprint(f\"NaN Ratio (0-100)\\n\\n{a}\")\n",
        "else:\n",
        "\tprint(colored(\"Now There is no NaN value in our Data\", 'red'))\n",
        "#===\n",
        "# ----------------------------------------------- Imputing Missing values\n",
        "# ------------------------------------ Numerical columns imputing\n",
        "if df.select_dtypes(\"number\").isna().sum().sum():\n",
        "\tnew_line()\n",
        "\tprint(f'(Before Missing values treatment)\\nThere are {df.isna().sum().sum()} Missing values:\\n\\t{df.select_dtypes(\"O\").isna().sum().sum()} in catagorical variables\\n\\t{df.select_dtypes(\"number\").isna().sum().sum()} in numerical columns\\n\\t{df.select_dtypes(exclude=[\"O\", \"number\"]).isna().sum().sum()} in others')\n",
        "\tfrom sklearn.impute import KNNImputer\n",
        "\tdf_not_a_number  = df.select_dtypes(exclude=\"number\")\n",
        "\tdf_number        = df.select_dtypes(\"number\")\n",
        "\tdel df\n",
        "\timputer = KNNImputer(n_neighbors=4, weights=\"uniform\")\n",
        "\timputed = imputer.fit_transform(df_number)\n",
        "\tdf_number = pd.DataFrame(imputed, columns=df_number.columns)\n",
        "\tdf = pd.concat([df_not_a_number.reset_index(drop=True), df_number.reset_index(drop=True)], axis=1)\n",
        "\tdel df_not_a_number\n",
        "\tdel df_number\n",
        "\tprint(f'\\n(After filling numeric missing values)\\nThere are {df.isna().sum().sum()} Missing values:\\n\\t{df.select_dtypes(\"O\").isna().sum().sum()} in catagorical variables\\n\\t{df.select_dtypes(\"number\").isna().sum().sum()} in numerical columns\\n\\t{df.select_dtypes(exclude=[\"O\", \"number\"]).isna().sum().sum()} in others')\n",
        "#===\n",
        "# -------------------------------- Catagoriacal variables imputating\n",
        "vars_to_fill = df.select_dtypes(\"O\").isna().mean().where(lambda x:x>0).dropna().sort_values(ascending=True)\n",
        "if vars_to_fill.size:\n",
        "\tfor col in vars_to_fill.index:\n",
        "\t\ttr = pd.concat([df[[col]], df.loc[:,df.isna().sum() == 0]], 1)\n",
        "\t\ttr_y = tr[col]\n",
        "\t\ttr_X = tr.drop(columns=col)\n",
        "\n",
        "\t\ttr_T = tr_X.select_dtypes(\"number\")\n",
        "\t\tcat_cols = pd.get_dummies(tr_X.select_dtypes(exclude=\"number\"), prefix_sep=\"__\")\n",
        "\t\ttr_T[cat_cols.columns.to_list()] = cat_cols\n",
        "\n",
        "\t\ttr_T[col] = tr_y\n",
        "\t\ttr = tr_T.copy(\"deep\")\n",
        "\n",
        "\t\ttrain = tr[tr[col].notna()]\n",
        "\t\ttest  = tr[tr[col].isna()]\n",
        "\n",
        "\t\ttrain_y = train[col]\n",
        "\t\ttrain_X = train.drop(columns=col)\n",
        "\n",
        "\t\ttest_X = test.drop(columns=col)\n",
        "\n",
        "\t\tclf = DecisionTreeClassifier().fit(train_X, train_y)\n",
        "\t\ttest_y = clf.predict(test_X)\n",
        "\n",
        "\t\tdf.loc[df[col].isna(), col] = test_y\n",
        "\tnew_line()\n",
        "\tprint(f\"Missing values imputed, Now there are {df.isna().sum().sum()} Missing values\")\n",
        "# ----------------------------------------------- END Imputing Missing values\n",
        "# --------------------------------------------------------- Unique values\n",
        "only_one_unique_value = df.nunique().where(lambda x:x == 1).dropna()\n",
        "if only_one_unique_value.size:\n",
        "\tnew_line()\n",
        "\tdf.drop(columns=only_one_unique_value.index, inplace=True)\n",
        "\tlast_ = (\"\", \"it\") if  only_one_unique_value.size == 1 else (\"s\", \"those\")\n",
        "\tto_print = f\"There are {only_one_unique_value.size} variable{last_[0]} That have only one unique value, so we droped {last_[1]}.\\nDropped column{last_[0]} name{last_[0]} (in order):\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\tfor i in only_one_unique_value.index.sort_values():\n",
        "\t\tprint(i)\n",
        "\tnew_line()\n",
        "\tprint(f\"\\nNow {data_shape()}\")\n",
        "del only_one_unique_value\n",
        "# #===\n",
        "all_values_are_unique = df.apply(lambda x:x.is_unique).where(lambda x:x==True).dropna()\n",
        "if all_values_are_unique.size:\n",
        "\tnew_line()\n",
        "\tdf.drop(columns=all_values_are_unique.index, inplace=True)\n",
        "\tlast_ = (\"\", \"it\") if  all_values_are_unique.size == 1 else (\"s\", \"those\")\n",
        "\tto_print = f\"There are {all_values_are_unique.size} column{last_[0]} that have all unique values, so no value repeatation, we droped {last_[1]} column{last_[0]}.\\nDropped column{last_[0]} name{last_[0]} are:\\n\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\tfor i in all_values_are_unique.index:\n",
        "\t\tprint(\"\\t\", i)\n",
        "\tnew_line()\n",
        "\tprint(f\"Now {data_shape()}\")\n",
        "del all_values_are_unique\n",
        "#===\n",
        "date_columns = []\n",
        "def DTYPES():\n",
        "\tglobal date_columns\n",
        "\tcatagorical_columns = df.head().select_dtypes(\"O\").columns\n",
        "\tnumerical_columns   = df.head().select_dtypes(\"number\").columns\n",
        "\tdate_columns        = []\n",
        "\n",
        "\tfor i in catagorical_columns:\n",
        "\t\ttry:\n",
        "\t\t\tdf[i] = pd.to_datetime(df[i])\n",
        "\t\t\tdate_columns.append(i)\n",
        "\t\texcept:\n",
        "\t\t\tpass\n",
        "\n",
        "\tcatagorical_columns = catagorical_columns.drop(date_columns)\n",
        "\tif date_columns:\n",
        "\t\tdate_columns = pd.Index(date_columns)\n",
        "\t#===\n",
        "\tif not catagorical_columns.append(numerical_columns).append(date_columns).is_unique:\n",
        "\t\tnew_line()\n",
        "\t\tprint(colored(\"Some column/s repated in > 1 dtypes\\n\", 'red'))\n",
        "\t\tdtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "\t\t\t\t\t\"dtype\" : ['O']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "\t\tprint(dtypes[dtypes.Column.isin(list(dtypes[dtypes.Column.duplicated()].Column.values))].to_string())\n",
        "\t#===\n",
        "\tx = df.columns.difference(\n",
        "\t\tcatagorical_columns.append(numerical_columns).append(date_columns)\n",
        "\t\t)\n",
        "\tif x.size:\n",
        "\t\tnew_line()\n",
        "\t\tprint(colored(\"Some columns not included in any existing catagory, those:\\n\", 'red'))\n",
        "\t\tfor i in x:\n",
        "\t\t\tprint(f\"\\t<{i}, with dtype of <{df[i].dtype}>\")\n",
        "\t#===\n",
        "\tdtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "\t\t\t\t\"dtype\" : ['Object']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "\treturn dtypes\n",
        "#===\n",
        "dtypes = DTYPES()\n",
        "# ----------------------------------------------------------------------- Feature enginearing\n",
        "# ======= Adding date columns\n",
        "# ?????????????????????????? add polynomial, sqrt, tree, log features\n",
        "len_df_before_adding_date_vars = df.shape[1]\n",
        "for date_col in date_columns:\n",
        "\tdf = add_new_date_cols(df[date_col], date_col)\n",
        "len_df_after_adding_date_vars  = df.shape[1]\n",
        "if len_df_after_adding_date_vars > len_df_before_adding_date_vars:\n",
        "\tnew_line()\n",
        "\tto_print = f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} date Features\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "\n",
        "# ======= type casting of numerical variable (those who have < 4% unique values) to catagorical variables\n",
        "f = (df.select_dtypes(\"number\").nunique() / len(df) * 100).where(lambda x:x<4).dropna().index\n",
        "if f.size:\n",
        "\tlen_df_before_adding_date_vars = df.shape[1]\n",
        "\tfor col_num_to_str in f:\n",
        "\t\tdf[col_num_to_str+\"_str\"] = '\"' + df[col_num_to_str].astype(str) + '\"'\n",
        "\tlen_df_after_adding_date_vars  = df.shape[1]\n",
        "\tnew_line()\n",
        "\tto_print = f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} String Features (Extracted from numerical variables)\"\n",
        "\tprint(colored(to_print, 'red'))\n",
        "# =======\n",
        "for var in df.select_dtypes(\"O\").columns:\n",
        "\tm = cluping_rare_cases_in_one_catagory(var)\n",
        "\tif isinstance(m, pd.core.series.Series):\n",
        "\t\tdf[var] = m\n",
        "new_line()\n",
        "\n",
        "xx = (df == 'Rare cases').sum().sort_values().where(lambda x:x>0).dropna()\n",
        "xx = pd.DataFrame({\"Count\" : xx,\n",
        "\t\t\t\t\"Ratio\" : round(xx/len(df)*100, 4)})\n",
        "print(f\"<Rare case> catagory:\\n{xx.to_string()}\")\n",
        "# ----------------------------------------------------------------------- END (Feature enginearing)\n",
        "dtypes = DTYPES()\n",
        "# ---------------------------------------------------- Correlation plot\n",
        "new_line()\n",
        "cor_df = df.select_dtypes('number').corr().abs()\n",
        "mask = np.triu(np.ones_like(cor_df, dtype=bool));\n",
        "f, ax = plt.subplots(figsize=(17, 10));\n",
        "cmap = sns.color_palette(\"viridis\", as_cmap=True);\n",
        "plot_ = sns.heatmap(cor_df, mask=mask, cmap=cmap, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
        "plot_.axes.set_title(\"abs (Correlation) plot\",fontsize=25);\n",
        "plt.show()\n",
        "# ---------------------------------------------------------------------\n",
        "#===\n",
        "if summary__:\n",
        "\tfor row in dtypes.iterrows():\n",
        "\t\tcolumn_name, type_ = row[1]\n",
        "\t\tx = df[column_name]\n",
        "\t\tto_print = f\"\\n\\n\\n========================================= {column_name} =========================================\\n\\n\"\n",
        "\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\tfor col_ in df.columns:\n",
        "\t\t\tif col_ == column_name:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tif df[col_].nunique() == df[column_name].nunique():\n",
        "\t\t\t\tunique_combination = df[[col_, column_name]].drop_duplicates()\n",
        "\t\t\t\tif unique_combination.apply(lambda x:x.is_unique).sum() == 2:\n",
        "\t\t\t\t\tnew_line()\n",
        "\t\t\t\t\tto_print = f\"This Columns is duplicate of <{col_}> column\"\n",
        "\t\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\tprint(f\"Column Type     : \", end=\"\")\n",
        "\t\tprint(colored(type_, 'red'))\n",
        "\t\tif x.isna().all():\n",
        "\t\t\tnew_line()\n",
        "\t\t\tdf.drop(columns=column_name, inplace=True)\n",
        "\t\t\tprint(colored(\"We dropped This column, because it is all Empty\", 'red'))\n",
        "\t\t\tcontinue\n",
        "\t\tif type_ in [\"O\", \"Date\"]:\n",
        "\t\t\tif x.is_unique:\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tdf.drop(columns=column_name, inplace=True)\n",
        "\t\t\t\tto_print = f\"We dropped This column, because it's a {type_} columns, and it's all values are unique\"\n",
        "\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\t\t\t\tcontinue\n",
        "\t\tif x.nunique() == 1:\n",
        "\t\t\tnew_line()\n",
        "\t\t\tdf.drop(columns=column_name, inplace=True)\n",
        "\t\t\tprint(colored(\"We dropped This column, because There is only one unique value\", 'red'))\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif type_ == \"Number\":\n",
        "\t\t\tlocal_cor = cor_df[column_name].drop(column_name).reset_index()\n",
        "\t\t\tlocal_cor = local_cor.reindex(local_cor[column_name].abs().sort_values().index)\n",
        "\t\t\tif local_cor[column_name].max() == 1:\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tto_print = f\"This column is perfactly correlated with column <{local_cor[local_cor[column_name] == 1]['index'].values[0]}, so remove one of them\"\n",
        "\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\txm = local_cor[-3:].rename(columns={'index' : 'Column name', column_name : 'Correlation'}).reset_index(drop=True)\n",
        "\t\t\txm.index = xm['Column name']\n",
        "\t\t\txm.drop(columns=\"Column name\", inplace=True);\n",
        "\t\t\txm.plot(kind='barh', grid=True, figsize=(10,1.5));\n",
        "\t\t\tplt.title(\"Most 3 correlated features with this columns (sorted)\", size=14);\n",
        "\t\t\tplt.xlabel(\"Correlation\", size=12);\n",
        "\t\t\tplt.show();\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\tskewness = x.skew(skipna = True)\n",
        "\t\t\tif abs(skewness) < 0.5:\n",
        "\t\t\t\tprint(f\"The data is fairly symmetrical (skewness is: {skewness})\")\n",
        "\t\t\telif abs(skewness) < 1:\n",
        "\t\t\t\tprint(f\"The data are moderately skewed (skewness is: {skewness})\")\n",
        "\t\t\telse:\n",
        "\t\t\t\tto_print = f\"The data are highly skewed (skewness is: {skewness})\\nNote: When skewness exceed |1| we called it highly skewed\"\n",
        "\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\t\t# f = x.describe()\n",
        "\t\t\t# f['Nunique'] = x.nunique()\n",
        "\t\t\t# f['Nunique ratio'] = f.loc[\"Nunique\"] / f.loc[\"count\"] * 100\n",
        "\t\t\t# f['Outlies count'] = (((x - x.mean())/x.std()).abs() > 3).sum()\n",
        "\t\t\t# f['Outlies ratio'] = f.loc[\"Outlies count\"] / f.loc[\"count\"] * 100\n",
        "\t\t\t# f['Nagative values count'] = (x < 0).sum()\n",
        "\t\t\t# f['Nagative values ratio'] = f['Nagative values count'] / f['count'] * 100\n",
        "\n",
        "\t\t\tff = [x.count(), x.isna().sum(), x.mean(), x.std(), x.min()]\n",
        "\t\t\tff += x.quantile([.25,.5,.75]).to_list()\n",
        "\t\t\tff += [x.max(), x.nunique(), (((x - x.mean())/x.std()).abs() > 3).sum(), (x < 0).sum(), (x == 0).sum()]\n",
        "\n",
        "\t\t\tf = pd.DataFrame(ff, index=['Count', 'NA', 'Mean', 'Std', 'Min', '25%', '50%', '75%', 'Max', 'Nunique', 'Outlies', 'Nagetive', 'Zeros'], columns=['Count'])\n",
        "\t\t\tf['Ratio'] = f.Count / x.count() * 100\n",
        "\t\t\tf.loc['Mean' : 'Max', 'Ratio'] = None\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\tprint(f.round(2).to_string())\n",
        "\t\t\tplot_numerical_columns(column_name)\n",
        "\n",
        "\t\telif type_ == \"Object\":\n",
        "\t\t\t# f = x.describe()\n",
        "\t\t\t# f = x.agg(['count', pd.Series.nunique])\n",
        "\t\t\t# f['len'] = len(x)\n",
        "\t\t\t# f['Na count'] = x.isna().sum()\n",
        "\t\t\t# f['Na ratio'] = f['Na count'] / f['count'] * 100\n",
        "\t\t\t# f['Most frequent'] = x.mode().values[0]\n",
        "\t\t\t# f['Most frequent count'] = (x == f['Most frequent']).sum()\n",
        "\t\t\t# f['Most frequent ratio'] = f['Most frequent count'] / f['count'] * 100\n",
        "\t\t\t# f['Least frequent'] = x.value_counts().tail(1).index[0]\n",
        "\t\t\t# f['Least frequent count'] = (x == f['Least frequent']).sum()\n",
        "\t\t\t# f['Least frequent ratio'] = f['Least frequent count'] / f['count'] * 100\n",
        "\t\t\t# f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "\t\t\t# f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "\t\t\tl = x.count(), x.nunique(), len(x), x.isna().sum(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "\t\t\tf = pd.DataFrame(l, index=['Count', 'Nunique', 'Len', 'NA', 'Most frequent', 'Least frequent', 'Values occured only once'], columns=['Counts'])\n",
        "\t\t\tf['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "\t\t\tf.loc[['Len'], 'Ratio'] = None\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\tprint(f.to_string())\n",
        "\n",
        "\n",
        "\t\t\tif x.str.lower().nunique() != x.nunique():\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tto_print = f\"Case issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin lower verstion there are   {x.str.lower().nunique()} unique values.\\n\"\n",
        "\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\t\tif x.str.strip().nunique() != x.nunique():\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tto_print = f\"Space issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin striped verstion there are {x.str.strip().nunique()} unique values.\"\n",
        "\t\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\t\tplot_catagorical_columns(column_name)\n",
        "\n",
        "\t\telif type == \"Date\":\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\trd = relativedelta.relativedelta( pd.to_datetime(x.max()), pd.to_datetime(x.min()))\n",
        "\t\t\tto_print = f\"Diffrenece between first and last date:\\n\\tYears : {rd.years}\\n\\tMonths: {rd.months}\\n\\tDays  : {rd.days}\"\n",
        "\t\t\tprint(colored(to_print, 'red'))\n",
        "\n",
        "\t\t\t# f = pd.Series({'Count' : x.count(),\n",
        "\t\t\t#             'Nunique count' : x.nunique(),\n",
        "\t\t\t#             'Nunique ratio' : x.nunique() / x.count() * 100,\n",
        "\t\t\t#             'Most frequent value' : str(x.mode()[0]),\n",
        "\t\t\t#             'Least frequent value' :  x.value_counts().tail(1).index[0]\n",
        "\t\t\t#             })\n",
        "\t\t\t# f['Most frequent count'] = (x == f['Most frequent value']).sum()\n",
        "\t\t\t# f['Most frequent ratio'] = f['Most frequent count'] / f['Count'] * 100\n",
        "\t\t\t# f['Least frequent count'] = (x == f['Least frequent value']).sum()\n",
        "\t\t\t# f['Least frequent ratio'] = f['Least frequent count'] / f['Count'] * 100\n",
        "\t\t\t# f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "\t\t\t# f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "\t\t\tff = x.count(), x.nunique(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "\t\t\tf = pd.DataFrame(ff, index=[\"Count\", 'Nunique', 'Most frequent values', 'Least frequent values', 'Values occured only once count'], columns=['Counts'])\n",
        "\t\t\tf['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\tprint(f\"\\n{f.to_string()}\")\n",
        "\n",
        "\n",
        "\t\t\tf = set(np.arange(x.dt.year.min(),x.dt.year.max()+1)).difference(\n",
        "\t\t\t\tx.dt.year.unique())\n",
        "\t\t\tif f:\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tprint(colored(\"These Years (in order) are missing:\\n\", 'red'))\n",
        "\t\t\t\tfor i in f:\n",
        "\t\t\t\t\tprint(\"\\t\", i, end=\", \")\n",
        "\n",
        "\t\t\tf = set(np.arange(x.dt.month.min(),x.dt.month.max()+1)).difference(\n",
        "\t\t\t\tx.dt.month.unique())\n",
        "\t\t\tif f:\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tprint(colored(\"These Months (in order) are missing:\\n\", 'red'))\n",
        "\t\t\t\tfor i in f:\n",
        "\t\t\t\t\tprint(\"\\t\", i, end=\", \")\n",
        "\n",
        "\t\t\tf = set(np.arange(x.dt.day.min(),x.dt.day.max()+1)).difference(\n",
        "\t\t\t\tx.dt.day.unique())\n",
        "\t\t\tif f:\n",
        "\t\t\t\tnew_line()\n",
        "\t\t\t\tprint(colored(\"These Days (in order) are missing:\\n\", 'red'))\n",
        "\t\t\t\tfor i in f:\n",
        "\t\t\t\t\tprint(\"\\t\", i, end=\", \")\n",
        "\n",
        "\t\t\tnew_line()\n",
        "\t\t\tplot_date_columns(column_name)\n",
        "\n",
        "pickle.dump(df, open(\"df.pkl\", \"wb\"))\n",
        "df.to_csv(\"df.csv\", index=False)\n",
        "target_variable = \"SalePrice\"\n",
        "open(\"target_variable.txt\", \"w\").write(target_variable)\n",
        "# ================================================================================================================ Modeling\n",
        "# if modeling_:\n",
        "# \t# df = pickle.load(open(\"df.pkl\", \"rb\"))\n",
        "# \tprint(\"\\n\\n\")\n",
        "# \tto_print = \"----------------------------------------------------------------------------------------------\\n****************************************** Modeling ******************************************\"\n",
        "# \tprint(colored(to_print, 'red'))\n",
        "#\n",
        "# \t# Regression problem\n",
        "# \tif df[target_variable].dtype in [float, int]:\n",
        "#\n",
        "# \t\tto_print = \"\\n-------------------- This is Regression problem --------------------\\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"\n",
        "# \t\tprint(colored(to_print, 'red'))\n",
        "#\n",
        "# \t\tdf_T = df.select_dtypes(\"number\")\n",
        "# \t\tcat_cols = pd.get_dummies(df.select_dtypes(exclude=\"number\"), prefix_sep=\"__\")\n",
        "# \t\tdf_T[cat_cols.columns.to_list()] = cat_cols\n",
        "#\n",
        "# \t\tdf = df_T.copy(\"deep\")\n",
        "# \t\tdel df_T\n",
        "# \t\tdel cat_cols\n",
        "# \t\t# ====\n",
        "# \t\ttrain_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "# \t\t# ====\n",
        "# \t\t# --------------------------------------------------------- Linear regression\n",
        "# \t\tto_print = \"\\n ------------------------------------- Linear Regression -------------------------------------\\n\"\n",
        "# \t\tprint(colored(to_print, 'red'))\n",
        "#\n",
        "# \t\t# print(\"\\nStarting Feature selection for Linear regression........\")\n",
        "# \t\t# selector = SelectFromModel(estimator=LinearRegression()).fit(train_X, train_y).get_support(True)\n",
        "# \t\t# print(f\"Droppped {(selector == False).sum()} useless features.\\n\")\n",
        "# \t\t# train_X = train_X.iloc[:, selector]\n",
        "# \t\t# test_X  = test_X.iloc [:, selector]\n",
        "#\n",
        "# \t\t# model = LinearRegression()\n",
        "# \t\t# rfe = RFE(model, 7)\n",
        "# \t\t# X_rfe = rfe.fit_transform(train_X,train_y)\n",
        "# \t\t# model.fit(X_rfe,train_y)\n",
        "# \t\t# rfe.support_\n",
        "# \t\t# rfe.ranking_\n",
        "#\n",
        "#\n",
        "# \t\tmodel_reg = OLS(train_y, train_X).fit()\n",
        "# \t\tsummary = model_reg.summary()\n",
        "# \t\tsummary_df = pd.DataFrame(summary.tables[1])\n",
        "# \t\tsummary_df.columns = summary_df.iloc[0]\n",
        "# \t\tsummary_df.drop(0, inplace=True)\n",
        "# \t\tsummary_df.columns = summary_df.columns.astype(str)\n",
        "# \t\tsummary_df.columns = [\"Variable\"] + summary_df.columns[1:].to_list()\n",
        "# \t\tfor i in summary_df.columns[1:]:\n",
        "# \t\t\tsummary_df[i] = summary_df[i].astype(str).astype(float)\n",
        "# \t\tsummary_df.Variable = summary_df.Variable.astype(str)\n",
        "# \t\tsummary_df['Indicator'] = summary_df['P>|t|'].apply(lambda x:\"***\" if x < 0.001 else \"**\" if x < 0.01 else \"*\" if x < 0.05 else \".\" if x < 0.1  else \"\")\n",
        "# \t\tsummary_df = summary_df.sort_values(\"Variable\").reset_index(drop=True)\n",
        "# \t\tsummary_df.to_csv()\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(colored(\"NOTE: This summary saved as <summary_OLS_1.csv>\", 'red'))\n",
        "#\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(summary_df.to_string())\n",
        "# \t\t# ============================= Model statistic\n",
        "# \t\tpredictions = model_reg.predict(test_X)\n",
        "#\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(colored(\" --- Model statistic --- \\n\", 'red'))\n",
        "# \t\tprint(f\"R-squared         : {round(model_reg.rsquared, 3)}\")\n",
        "# \t\tprint(f\"Adj. R-squared    : {round(model_reg.rsquared_adj, 3)}\")\n",
        "# \t\tprint(f\"F-statistic       : {round(model_reg.fvalue)}\")\n",
        "# \t\tprint(f\"Prob (F-statistic): {model_reg.f_pvalue}\")\n",
        "# \t\tprint(f\"No. Observations  : {round(model_reg.nobs)}\")\n",
        "# \t\tprint(f\"AIC               : {round(model_reg.aic)}\")\n",
        "# \t\tprint(f\"Df Residuals      : {round(model_reg.df_resid)}\")\n",
        "# \t\tprint(f\"BIC               : {round(model_reg.bic)}\")\n",
        "# \t\tprint(f\"RMSE (test)       : {RMSE(predictions)}\")\n",
        "# \t\t# ======\n",
        "# \t\tf = train_X.copy(\"deep\")\n",
        "# \t\tf['Errors__'] = model_reg.resid\n",
        "# \t\tf = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "# \t\tprint(f\"Mean of train reseduals: {model_reg.resid.mean()}\")\n",
        "# \t\tdel f\n",
        "# \t\t# ============================= END (Model statistic)\n",
        "# \t\t# --------------------------------------------------------- END Linear regression\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# \t\t# --------------------------------------------------------- Random Forest\n",
        "# \t\tprint(\"\\n ------------------------------------- Random Forest -------------------------------------\\n\")\n",
        "#\n",
        "# \t\trf = RandomForestRegressor(n_estimators = 200, oob_score=True)\n",
        "# \t\tmodel_rf = rf.fit(train_X, train_y);\n",
        "# \t\tpredictions_rf = rf.predict(test_X)\n",
        "#\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(colored(\"RF model peramters:\\n\", 'red'))\n",
        "# \t\tpprint.pprint(model_rf.get_params())\n",
        "#\n",
        "# \t\tnew_line()\n",
        "# \t\timportances = list(rf.feature_importances_)\n",
        "# \t\tfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(test_X, importances)]\n",
        "# \t\tfeaturesImportance = pd.Series(model_rf.feature_importances_, index=train_X.columns).sort_values(ascending=False)\n",
        "# \t\tif len(featuresImportance) > 30:\n",
        "# \t\t\tfeaturesImportance = featuresImportance.head(30)\n",
        "# \t\tfeaturesImportance.plot(figsize=(20,10), kind='bar', grid=True);\n",
        "# \t\tplt.title(\"RandomForest Feature importances Graph\", size=18,color='red');\n",
        "# \t\tplt.xlabel(\"Features\", size=14, color='red');\n",
        "# \t\tplt.ylabel(\"Importance\", size=14, color='red');\n",
        "# \t\tplt.show();\n",
        "# \t\tdel featuresImportance\n",
        "#\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(colored(\"--- Model statistic ---\", 'red'))\n",
        "# \t\t# The coefficient of determination R^2 of the prediction.\n",
        "# \t\t# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "# \t\tprint(f\"R^2 (test) : {rf.score(test_X, test_y)}\")\n",
        "# \t\tprint(f\"R^2 (train): {rf.score(train_X, train_y)}\")\n",
        "# \t\tprint(f\"RMSE (test): {RMSE(predictions_rf)}\")\n",
        "# \t\tprint(f\"oob score  : {model_rf.oob_score_}\")\n",
        "#\n",
        "# \t\tf = test_X.copy(\"deep\")\n",
        "# \t\terrors_rf = predictions_rf - test_y\n",
        "# \t\tf['Errors__'] = errors_rf\n",
        "# \t\tf = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "# \t\tnew_line()\n",
        "# \t\tprint(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "# \t\t# --------------------------------------------------------- END Random Forest\n",
        "# \telif df[target_variable].dtype == \"O\":\n",
        "# \t\t# Classififcation problem\n",
        "# \t\tif df[target_variable].nunique() == 2:\n",
        "# \t\t\tprint(\"\\n-------------------- This is Binary Classification problem --------------------\\n\")\n",
        "# \t\t\tprint(\"''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\")\n",
        "# \t\t\tdf = pd.concat([\n",
        "# \t\t\t\t\t\t\tdf.select_dtypes(exclude = \"O\"),\n",
        "# \t\t\t\t\t\t\tpd.get_dummies(df.drop(columns=target_variable).select_dtypes(\"O\")),\n",
        "# \t\t\t\t\t\t\tdf[[target_variable]]\n",
        "# \t\t\t\t\t\t\t], 1)\n",
        "#\n",
        "# \t\t\ttrain_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "# \t\t\tclf = LogisticRegression().fit(train_X, train_y)\n",
        "#\n",
        "# \t\t\t# Feture selection\n",
        "# \t\t\t# clf = LogisticRegression()\n",
        "# \t\t\t# rfe = RFE(clf, 3)\n",
        "# \t\t\t# clf = rfe.fit(train_X, train_y)\n",
        "#\n",
        "# \t\t\tpredictions = clf.predict_proba(test_X)\n",
        "# \t\t\tpredictions = pd.Series(predictions[:, 0])\n",
        "# \t\t\tlst = []\n",
        "# \t\t\tfor thresh in np.linspace(predictions.min(), predictions.max(), 50)[1:]:\n",
        "# \t\t\t\tpred = predictions < thresh\n",
        "#\n",
        "# \t\t\t\tpred.loc[pred == True] = clf.classes_[0]\n",
        "# \t\t\t\tpred.loc[pred == False] = clf.classes_[1]\n",
        "#\n",
        "# \t\t\t\ttest_y = test_y.reset_index(drop=True)\n",
        "#\n",
        "# \t\t\t\tTN = ((pred == clf.classes_[0]) & (test_y == clf.classes_[0])).sum()\n",
        "# \t\t\t\tTP = ((pred == clf.classes_[1]) & (test_y == clf.classes_[1])).sum()\n",
        "# \t\t\t\tFN = ((pred == clf.classes_[0]) & (test_y == clf.classes_[1])).sum()\n",
        "# \t\t\t\tFP = ((pred == clf.classes_[1]) & (test_y == clf.classes_[0])).sum()\n",
        "#\n",
        "# \t\t\t\tp = TP / (TP + FP)\n",
        "# \t\t\t\tr = TP / (TP + FN)\n",
        "# \t\t\t\tf = 2 * ((p * r) / (p+r))\n",
        "#\n",
        "# \t\t\t\tlst.append((thresh, (pred == test_y).mean(), p, r , f))\n",
        "#\n",
        "# \t\t\td = pd.DataFrame(lst, columns=[\"Thresold\", \"Accurecy(0-1)\", \"Precision\", \"Recall\", \"F1\"])\n",
        "# \t\t\td = d.set_index(\"Thresold\")\n",
        "# \t\t\td.plot(grid=True, figsize=(18,7));\n",
        "# \t\t\tplt.title(\"Model performance at diffrent Thresolds\", size=18, color='red');\n",
        "# \t\t\tplt.xlabel(\"Thresold\", size=14, color='red');\n",
        "# \t\t\tplt.ylabel(\"\");\n",
        "# \t\t\tplt.show()\n",
        "# \t\telse:\n",
        "# \t\t\tto_print = \"\\n-------------------- This is Multiclass Classification problem --------------------\\n\"\n",
        "# \t\t\tprint(colored(to_print, 'red'))\n",
        "# \t\t\tprint(\"'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\")\n",
        "#\n",
        "# \t\t\tdf.loc[:, df.select_dtypes(\"O\").columns] = df.select_dtypes(\"O\").apply(lambda x: pd.Series(LabelEncoder().fit_transform(x.astype(str))).astype(str))\n",
        "# \t\t\ttrain_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "#\n",
        "# \t\t\tclf=RandomForestClassifier(n_estimators=1000).fit(train_X, train_y)\n",
        "# \t\t\tpredictions = clf.predict(test_X)\n",
        "# \t\t\tfeature_imp = pd.Series(clf.feature_importances_,index=train_X.columns).sort_values(ascending=False)\n",
        "# \t\t\tif feature_imp.size > 30:\n",
        "# \t\t\t\tfeature_imp = feature_imp.head(30)\n",
        "# \t\t\tfeature_imp.plot(kind='barh', figsize=(17,10), grid=True);\n",
        "# \t\t\tplt.title(\"Feature importances Graph\", size=18, color='red');\n",
        "# \t\t\tplt.xlabel(\"Importance\", size=14, color='red');\n",
        "# \t\t\tplt.ylabel(\"Feature\", size=14, color='red');\n",
        "# \t\t\tplt.show()\n",
        "# \t\t\t# ====\n",
        "# \t\t\tf = (test_y, predictions)\n",
        "# \t\t\tf_int = (test_y.astype(int), predictions.astype(int))\n",
        "#\n",
        "# \t\t\tprint(f\"accuracy_score: {metrics.accuracy_score(*f)}\")\n",
        "# \t\t\tprint(f\"f1_score: {metrics.f1_score(*f_int)}\")\n",
        "#\n",
        "# \t\t\tmetrics.plot_roc_curve(clf, test_X, test_y);\n",
        "# \t\t\tplt.title(\"ROC curve plot\");\n",
        "# \t\t\tplt.show();\n",
        "#\n",
        "# \t\t\tmetrics.ConfusionMatrixDisplay(metrics.confusion_matrix(*f)); plt.show()\n",
        "#\n",
        "# \t\t\tmetrics.plot_confusion_matrix(clf, test_X, test_y);\n",
        "# \t\t\tplt.title(\"Confusion matrix\");\n",
        "# \t\t\tplt.show()\n",
        "#\n",
        "# \t\t\tmetrics.plot_precision_recall_curve(clf, test_X, test_y);\n",
        "# \t\t\tplt.title(\"Precision recall curve\");\n",
        "# \t\t\tplt.show()\n",
        "# \t# ================================================================================================================ END Modeling\n",
        "#\n",
        "# # sklearn.feature_selection\n",
        "# # <SelectKBest>             removes all but the highest scoring features\n",
        "# #     from sklearn.feature_selection import SelectKBest\n",
        "# #     from sklearn.feature_selection import chi2\n",
        "# #     X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
        "# # <SelectPercentile>        removes all but a user-specified highest scoring percentage of features\n",
        "# # <GenericUnivariateSelect> allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# # pickle.dump(df, open(\"df.pkl\", \"wb\"))\n",
        "# df = pickle.load(open(\"df.pkl\", \"rb\"))\n",
        "# target_variable = \"SalePrice\"\n",
        "# train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "#\n",
        "# def select_best_var(model_reg):\n",
        "# \tsummary = model_reg.summary()\n",
        "# \tsummary_df = pd.DataFrame(summary.tables[1])\n",
        "# \tsummary_df.columns = summary_df.iloc[0]\n",
        "# \tsummary_df.drop(0, inplace=True)\n",
        "# \tsummary_df.columns = summary_df.columns.astype(str)\n",
        "# \tsummary_df.columns = [\"Variable\"] + summary_df.columns[1:].to_list()\n",
        "# \tfor i in summary_df.columns[1:]:\n",
        "# \t\tsummary_df[i] = summary_df[i].astype(str).astype(float)\n",
        "# \tsummary_df.Variable = summary_df.Variable.astype(str)\n",
        "# \treturn summary.tables[0].data, summary_df\n",
        "# \t# summary_df['Indicator'] = summary_df['P>|t|'].apply(lambda x:\"***\" if x < 0.001 else \"**\" if x < 0.01 else \"*\" if x < 0.05 else \".\" if x < 0.1  else \"\")\n",
        "# \t# summary_df = summary_df.sort_values(\"Variable\").reset_index(drop=True)\n",
        "#\n",
        "#\n",
        "# FINAL_VARS = []\n",
        "# best_adj_rsquared = 0\n",
        "# FM = []\n",
        "# for col in train_X.columns:\n",
        "# \tmodel_reg = OLS(train_y, train_X[col]).fit()\n",
        "# \ts , summary_df= select_best_var(model_reg)\n",
        "# \tadj_rsquared = float(s[1][-1].strip())\n",
        "# \tvar = summary_df.loc[summary_df['P>|t|'].idxmax()]['Variable']\n",
        "# \tFM.append((adj_rsquared, var))\n",
        "#\n",
        "# selected = pd.DataFrame(FM, columns=['Adj_rsquared', 'Var']).sort_values('Adj_rsquared').tail(1)\n",
        "# Adj_rsquared = selected.Adj_rsquared.iloc[0]\n",
        "# first_var = selected['Var'].values[0]\n",
        "# FINAL_VARS.append(first_var)\n",
        "# k = 0\n",
        "# cols_used = []\n",
        "# for upper_column in train_X.columns:\n",
        "#\n",
        "# \tif upper_column in FINAL_VARS:\n",
        "# \t\tcontinue\n",
        "# \tFM = []\n",
        "# \tfor i in train_X.columns:\n",
        "# \t\tk += 1\n",
        "# \t\tif i in FINAL_VARS:\n",
        "# \t\t\tcontinue\n",
        "# \t\tif i == upper_column:\n",
        "# \t\t\tcontinue\n",
        "# \t\tif i in cols_used:\n",
        "# \t\t\tcontinue\n",
        "# \t\tcols_used.append(i)\n",
        "# \t\tmodel_reg = OLS(train_y, train_X[FINAL_VARS + [i]]).fit()\n",
        "# \t\ts , summary_df= select_best_var(model_reg)\n",
        "# \t\tadj_rsquared = float(s[1][-1].strip())\n",
        "# \t\tFM.append((summary_df[summary_df.Variable == i]['P>|t|'].iloc[0], i))\n",
        "# \t\tbest_cendidate = pd.DataFrame(FM, columns=['Adj_rsquared', 'Var']).sort_values('Adj_rsquared').tail(1)\n",
        "# \t\tif best_cendidate.Adj_rsquared.iloc[0] > best_adj_rsquared:\n",
        "# \t\t\tbest_adj_rsquared = best_cendidate.Adj_rsquared.iloc[0]\n",
        "# \t\t\tFINAL_VARS.append(\n",
        "# \t\t\t\tbest_cendidate['Var'].iloc[0]\n",
        "# \t\t\t\t)\n",
        "# \t\tprint(k, end=\",\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}