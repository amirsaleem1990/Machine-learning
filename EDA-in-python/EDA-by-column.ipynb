{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from termcolor import colored\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import missingno as msno\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "import pprint\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from pandas_profiling import ProfileReport\n",
        "from dateutil import relativedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "\n",
        "plot_______ = False\n",
        "\n",
        "def new_line():\n",
        "    print(\"\\n-------------------------\\n\")\n",
        "\n",
        "def RMSE(predictions):\n",
        "    return round(np.sqrt(((test_y - predictions)**2).mean()))\n",
        "\n",
        "def plot_numerical_columns(col_name):\n",
        "    if not plot_______:\n",
        "        return None\n",
        "\n",
        "    # Histogram\n",
        "    df[col_name].plot(kind=\"hist\", figsize=(13,8));\n",
        "    plt.title(col_name, size=18);\n",
        "    plt.axhline(y=df[col_name].mean(), color='red');\n",
        "    plt.axhline(y=df[col_name].median(), color='green');\n",
        "    plt.legend(['Actual', 'Mean', 'Median']);\n",
        "    plt.show()\n",
        "\n",
        "    # Scatter plot\n",
        "    df[col_name].plot(figsize=(13,8));\n",
        "    plt.title(col_name, size=18);\n",
        "    plt.axhline(y=df[col_name].mean(), color='red');\n",
        "    plt.axhline(y=df[col_name].median(), color='green');\n",
        "    plt.legend(['Actual', 'Mean', 'Median']);\n",
        "    plt.show()\n",
        "\n",
        "    # scatter plot (sort by values), values Vs index\n",
        "    df[col_name].sort_values().reset_index(drop=True).plot(figsize=(13,8));\n",
        "    plt.title(col_name+\" (SORTED)\", size=18);\n",
        "    plt.axhline(y=df[col_name].mean(), color='red');\n",
        "    plt.axhline(y=df[col_name].median(), color='green');\n",
        "    plt.legend(['Actual', 'Mean', 'Median']);\n",
        "    plt.show()\n",
        "\n",
        "    # box plot\n",
        "    df[col_name].plot(kind=\"box\", figsize=(13,8))\n",
        "    plt.title(col_name, size=18);\n",
        "    plt.xlabel(\"\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_date_columns(col_name):\n",
        "    if not plot_______:\n",
        "        return None\n",
        "    df[col_name].plot(figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Index\", size=14);\n",
        "    plt.ylabel(\"Date\", size=14);\n",
        "    plt.title(col_name + \" Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    df[col_name].sort_values().reset_index(drop=True).plot(figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Index (sorted)\", size=14);\n",
        "    plt.ylabel(\"Year\", size=14);\n",
        "    plt.title(col_name + \" Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.year.value_counts(sort=False).sort_index() / len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Year\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" year Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.month.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Month\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" month Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "    (df[col_name].dt.day.value_counts().sort_index()/len(df) * 100).plot(kind=\"bar\", figsize=(15,7), grid=True);\n",
        "    plt.xlabel(\"Day\", size=14);\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14);\n",
        "    plt.title(col_name + \" Day Frequency Graph\", size=18);\n",
        "    plt.show();\n",
        "\n",
        "def plot_catagorical_columns(cat_variable):\n",
        "    if not plot_______:\n",
        "        return None\n",
        "    (df[cat_variable].value_counts() / len(df) * 100).plot.bar(figsize=(15,6), grid=True);\n",
        "    plt.title(cat_variable, size=18, color='r');\n",
        "    plt.xlabel(\"Catagory\", size=14, color='r');\n",
        "    plt.ylabel(\"Ratio (1-100)\", size=14, color='r');\n",
        "    plt.show()\n",
        "\n",
        "def data_shape():\n",
        "    return f\"The Data have:\\n\\t{df.shape[0]} rows\\n\\t{df.shape[1]} columns\\n\"\n",
        "#===\n",
        "# df = pd.read_csv(\"data.csv\", date_parser=True)\n",
        "\n",
        "# df = pd.read_csv(\"df_only_selected_columns_using_PCA.csv\", date_parser=True)\n",
        "# target_variable = \"ACTUAL_WORTH\"\n",
        "# df = pd.concat([\n",
        "#         df.select_dtypes(\"number\").iloc[:, :3],\n",
        "#         df.select_dtypes(\"O\").iloc[:, :3],\n",
        "#         df.select_dtypes(exclude=[\"number\", \"O\"]),\n",
        "#         df[[target_variable]]], 1)\n",
        "# target_variable = \"AREA_NAME_EN\"\n",
        "\n",
        "# df = pd.read_csv(\"cleaned_data.csv\", date_parser=True)\n",
        "# target_variable = \"SalePrice\"\n",
        "\n",
        "train = pd.read_csv(\"/home/amir/Downloads/train.csv\")\n",
        "test  = pd.read_csv(\"/home/amir/Downloads/test.csv\")\n",
        "target_variable = \"SalePrice\"\n",
        "train_y = train.SalePrice\n",
        "train = train.drop(columns=target_variable)\n",
        "df = pd.concat([train, test])\n",
        "df[target_variable] = len(train_y.to_list() + [None]*len(test))\n",
        "\n",
        "#===\n",
        "new_line()\n",
        "print(data_shape())\n",
        "#===\n",
        "new_line()\n",
        "print(f\"Columns types distribution:\\n\\n{df.dtypes.value_counts()}\\n\")\n",
        "df.dtypes.value_counts().plot(kind='barh', figsize=(10, 2), grid=True, title=\"Variable types Count Graph\");\n",
        "plt.xlabel(\"Count\");\n",
        "\n",
        "#===\n",
        "f = df[target_variable].isna().sum()\n",
        "if f:\n",
        "    new_line()\n",
        "    to_print = f\"There are {f} NAs in target values, we droped those rows\"\n",
        "    print(colored(to_print, 'red'))\n",
        "    df = df[df[target_variable].notna()]\n",
        "del f\n",
        "#---------------------------------------------------\n",
        "# df.select_dtypes(\"O\").columns[:5]\n",
        "# D = df.select_dtypes(exclude=\"O\")\n",
        "# D2 = df.select_dtypes(\"O\").iloc[:,:5]\n",
        "# df = pd.concat([D, D2], 1)\n",
        "\n",
        "# profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
        "# profile.to_file(\"your_report.html\")\n",
        "#---------------------------------------- NA\n",
        "a = df.isna().sum().where(lambda x:x>0).dropna()\n",
        "if a.size:\n",
        "    new_line()\n",
        "    to_print = f\"There are {len(a)} (out of {df.shape[1]}, [{round(len(a)/df.shape[1]*100)}%]) columns that contains 1 or more NA.\"\n",
        "    print(colored(to_print, 'red'))\n",
        "\n",
        "    for i in a.index:\n",
        "        df[i+\"_NA_indicator\"] = df[i].isna().replace({True : \"Missing\", False : \"Not missing\"})\n",
        "    new_line()\n",
        "    to_print = f\"{a.size} NA_indicator variables added to the data\\n\"\n",
        "    print(colored(to_print, 'red'))\n",
        "\n",
        "\n",
        "    print(\"========= NA Graphs =========\\n\")\n",
        "    msno.matrix(df);\n",
        "    plt.title(\"NA Graph\");\n",
        "    plt.show()\n",
        "\n",
        "    new_line()\n",
        "    sns.heatmap(df.isnull(), cbar=False);\n",
        "    plt.title(\"NA Graph\");\n",
        "    plt.show()\n",
        "#===\n",
        "a = a.sort_values()/len(df)*100\n",
        "if (a == 100).sum():\n",
        "    new_line()\n",
        "    df.drop(columns=a[a==100].index, inplace=True)\n",
        "    to_print = f\"There are {(a == 100).sum()} columns that are all Missing values, so we droped those.\\nNow {data_shape()}\\n\\nDropped columns names:\"\n",
        "    print(colored(to_print, 'red'))\n",
        "    for i in a[a==100].index:\n",
        "        print(\"\\t\",i)\n",
        "    a = a[a != 100]\n",
        "#===\n",
        "x = df[a.index].dtypes.value_counts()\n",
        "if x.size:\n",
        "    new_line()\n",
        "    print(f\"NA columns data type Distribution:\\n\\n{x}\")\n",
        "del x\n",
        "#===\n",
        "new_line()\n",
        "if a.size:\n",
        "    print(f\"NaN Ratio (0-100)\\n\\n{a}\")\n",
        "else:\n",
        "    print(colored(\"Now There is no NaN value in our Data\", 'red'))\n",
        "#===\n",
        "# ----------------------------------------------- Imputing Missing values\n",
        "# ------------------------------------ Numerical columns imputing\n",
        "if df.select_dtypes(\"number\").isna().sum().sum():\n",
        "    new_line()\n",
        "    print(f'(Before Missing values treatment)\\nThere are {df.isna().sum().sum()} Missing values:\\n\\t{df.select_dtypes(\"O\").isna().sum().sum()} in catagorical variables\\n\\t{df.select_dtypes(\"number\").isna().sum().sum()} in numerical columns\\n\\t{df.select_dtypes(exclude=[\"O\", \"number\"]).isna().sum().sum()} in others')\n",
        "\n",
        "    from sklearn.impute import KNNImputer\n",
        "    df_not_a_number  = df.select_dtypes(exclude=\"number\")\n",
        "    imputer = KNNImputer(n_neighbors=4, weights=\"uniform\")\n",
        "    imputed = imputer.fit_transform(df.select_dtypes(\"number\"))\n",
        "    df = pd.DataFrame(imputed, columns=df.select_dtypes(\"number\").columns)\n",
        "    df = pd.concat([df, df_not_a_number], axis=1)\n",
        "    del df_not_a_number\n",
        "\n",
        "    print(f'\\n(After filling numeric missing values)\\nThere are {df.isna().sum().sum()} Missing values:\\n\\t{df.select_dtypes(\"O\").isna().sum().sum()} in catagorical variables\\n\\t{df.select_dtypes(\"number\").isna().sum().sum()} in numerical columns\\n\\t{df.select_dtypes(exclude=[\"O\", \"number\"]).isna().sum().sum()} in others')\n",
        "#===\n",
        "# -------------------------------- Catagoriacal variables imputating\n",
        "vars_to_fill = df.select_dtypes(\"O\").isna().mean().where(lambda x:x>0).dropna().sort_values(ascending=True)\n",
        "if vars_to_fill.size:\n",
        "    for col in vars_to_fill.index:\n",
        "        tr = pd.concat([df[[col]], df.loc[:,df.isna().sum() == 0]], 1)\n",
        "        tr_y = tr[col]\n",
        "        tr_X = tr.drop(columns=col)\n",
        "\n",
        "        tr_T = tr_X.select_dtypes(\"number\")\n",
        "        cat_cols = pd.get_dummies(tr_X.select_dtypes(exclude=\"number\"), prefix_sep=\"__\")\n",
        "        tr_T[cat_cols.columns.to_list()] = cat_cols\n",
        "\n",
        "        tr_T[col] = tr_y\n",
        "        tr = tr_T.copy(\"deep\")\n",
        "\n",
        "        train = tr[tr[col].notna()]\n",
        "        test  = tr[tr[col].isna()]\n",
        "\n",
        "        train_y = train[col]\n",
        "        train_X = train.drop(columns=col)\n",
        "\n",
        "        test_X = test.drop(columns=col)\n",
        "\n",
        "        clf = DecisionTreeClassifier().fit(train_X, train_y)\n",
        "        test_y = clf.predict(test_X)\n",
        "\n",
        "        df.loc[df[col].isna(), col] = test_y\n",
        "    new_line()\n",
        "    print(\"Missing values imputed, Now there are {df.isna().sum().sum()} Missing values\")\n",
        "# ----------------------------------------------- END Imputing Missing values\n",
        "# --------------------------------------------------------- Unique values\n",
        "only_one_unique_value = df.nunique().where(lambda x:x == 1).dropna()\n",
        "if only_one_unique_value.size:\n",
        "    new_line()\n",
        "    df.drop(columns=only_one_unique_value.index, inplace=True)\n",
        "    last_ = (\"\", \"it\") if  only_one_unique_value.size == 1 else (\"s\", \"those\")\n",
        "    to_print = f\"There are {only_one_unique_value.size} variable{last_[0]} That have only one unique value, so we droped {last_[1]}.\\nDropped column{last_[0]} name{last_[0]} (in order):\"\n",
        "    print(colored(to_print, 'red'))\n",
        "    for i in only_one_unique_value.index.sort_values():\n",
        "        print(i)\n",
        "    new_line()\n",
        "    print(f\"\\nNow {data_shape()}\")\n",
        "del only_one_unique_value\n",
        "# #===\n",
        "all_values_are_unique = df.apply(lambda x:x.is_unique).where(lambda x:x==True).dropna()\n",
        "if all_values_are_unique.size:\n",
        "    new_line()\n",
        "    df.drop(columns=all_values_are_unique.index, inplace=True)\n",
        "    last_ = (\"\", \"it\") if  all_values_are_unique.size == 1 else (\"s\", \"those\")\n",
        "    to_print = f\"There are {all_values_are_unique.size} column{last_[0]} that have all unique values, so no value repeatation, we droped {last_[1]} column{last_[0]}.\\nDropped column{last_[0]} name{last_[0]} are:\\n\"\n",
        "    print(colored(to_print, 'red'))\n",
        "    for i in all_values_are_unique.index:\n",
        "        print(\"\\t\", i)\n",
        "    new_line()\n",
        "    print(f\"Now {data_shape()}\")\n",
        "del all_values_are_unique\n",
        "#===\n",
        "date_columns = []\n",
        "def DTYPES():\n",
        "    global date_columns\n",
        "    catagorical_columns = df.head().select_dtypes(\"O\").columns\n",
        "    numerical_columns   = df.head().select_dtypes(\"number\").columns\n",
        "    date_columns        = []\n",
        "\n",
        "    for i in catagorical_columns:\n",
        "        try:\n",
        "            df[i] = pd.to_datetime(df[i])\n",
        "            date_columns.append(i)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    catagorical_columns = catagorical_columns.drop(date_columns)\n",
        "    if date_columns:\n",
        "        date_columns = pd.Index(date_columns)\n",
        "    #===\n",
        "    if not catagorical_columns.append(numerical_columns).append(date_columns).is_unique:\n",
        "        new_line()\n",
        "        print(colored(\"Some column/s repated in > 1 dtypes\\n\", 'red'))\n",
        "        dtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "                    \"dtype\" : ['O']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "        print(dtypes[dtypes.Column.isin(list(dtypes[dtypes.Column.duplicated()].Column.values))].to_string())\n",
        "    #===\n",
        "    x = df.columns.difference(\n",
        "        catagorical_columns.append(numerical_columns).append(date_columns)\n",
        "        )\n",
        "    if x.size:\n",
        "        new_line()\n",
        "        print(colored(\"Some columns not included in any existing catagory, those:\\n\", 'red'))\n",
        "        for i in x:\n",
        "            print(f\"\\t<{i}, with dtype of <{df[i].dtype}>\")\n",
        "    #===\n",
        "    dtypes = pd.DataFrame({\"Column\" : catagorical_columns.append(numerical_columns).append(date_columns),\n",
        "                \"dtype\" : ['Object']*len(catagorical_columns) + ['Number']*len(numerical_columns) + ['Date']*len(date_columns)})\n",
        "    return dtypes\n",
        "#===\n",
        "dtypes = DTYPES()\n",
        "# ----------------------------------------------------------------------- Feature enginearing\n",
        "# ======= Adding date columns\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> add polynomial, sqrt, tree, log features\n",
        "def add_new_date_cols(x, suffix):\n",
        "    d = {}\n",
        "    d[suffix + '_week_normalized'] = x.dt.week / 52\n",
        "    d[suffix + '_week_str'] = '\"' + x.dt.week.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_year_after_min_year'] = x.dt.year - x.dt.year.min()\n",
        "    d[suffix + '_year_str'] = '\"' + x.dt.year.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_day_name']  = x.dt.day_name()\n",
        "\n",
        "    d[suffix + '_day_after_min_date_str']  = '\"' + (x - x.min()).apply(lambda x: str(x).split()[0]) + '\"'\n",
        "\n",
        "    d[suffix + '_day_normalized'] = x.dt.day / 31\n",
        "\n",
        "    d[suffix + '_hour_normalized'] = x.dt.hour / 24\n",
        "    d[suffix + '_hour_str'] = '\"' + x.dt.hour.apply(lambda x:np.nan if np.isnan(x) else str(x).replace(\".0\", \"\")) + '\"'\n",
        "\n",
        "    d[suffix + '_month_name'] = x.dt.month_name()\n",
        "    d[suffix + '_month_normalized'] = x.dt.month/12\n",
        "    for k,v in d.items():\n",
        "        if v.nunique() > 1:\n",
        "            df[k] = v\n",
        "    return df.drop(columns=x.name)\n",
        "    # return df\n",
        "\n",
        "len_df_before_adding_date_vars = df.shape[1]\n",
        "for date_col in date_columns:\n",
        "    df = add_new_date_cols(df[date_col], date_col)\n",
        "len_df_after_adding_date_vars  = df.shape[1]\n",
        "if len_df_after_adding_date_vars > len_df_before_adding_date_vars:\n",
        "    new_line()\n",
        "    to_print = f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} date Features\"\n",
        "    print(colored(to_print, 'red'))\n",
        "\n",
        "# ======= type casting of numerical variable (those who have < 4% unique values) to catagorical variables\n",
        "f = (df.select_dtypes(\"number\").nunique() / len(df) * 100).where(lambda x:x<4).dropna().index\n",
        "if f.size:\n",
        "    len_df_before_adding_date_vars = df.shape[1]\n",
        "    for col_num_to_str in f:\n",
        "        df[col_num_to_str+\"_str\"] = '\"' + df[col_num_to_str].astype(str) + '\"'\n",
        "    len_df_after_adding_date_vars  = df.shape[1]\n",
        "    new_line()\n",
        "    to_print = f\"Added {len_df_after_adding_date_vars - len_df_before_adding_date_vars} String Features (Extracted from numerical variables)\"\n",
        "    print(colored(to_print, 'red'))\n",
        "# =======\n",
        "def cluping_rare_cases_in_one_catagory(x):\n",
        "    global df\n",
        "    x = df[x]\n",
        "    orignal  = x.copy(\"deep\")\n",
        "    xx = x.value_counts()\n",
        "    xx = xx[xx< 10].index.to_list()\n",
        "    x =  x.replace(xx , \"Rare cases\")\n",
        "    if x.value_counts()[-1] < 8:\n",
        "        x[x == \"Rare cases\"] = x.mode()[0] # agar \"Rare cases\" vali catogery me 8 sy bhi kam values hon to un ko most common value sy replace kar do\n",
        "    if x.nunique() == 1:\n",
        "        new_line()\n",
        "        to_print = f\"The column <{x.name}> have only one unique value, We droped it from the data.\"\n",
        "        print(colored(to_print, 'red'))\n",
        "        # return orignal\n",
        "        df.drop(columns=x.name, inplace=True)\n",
        "        return None\n",
        "    return x\n",
        "\n",
        "for var in df.select_dtypes(\"O\").columns:\n",
        "    m = cluping_rare_cases_in_one_catagory(var)\n",
        "    if isinstance(m, pd.core.series.Series):\n",
        "        df[var] = m\n",
        "new_line()\n",
        "\n",
        "\n",
        "xx = (df == 'Rare cases').sum().sort_values().where(lambda x:x>0).dropna()\n",
        "xx = pd.DataFrame({\"Count\" : xx,\n",
        "                \"Ratio\" : round(xx/len(df)*100, 4)})\n",
        "print(f\"<Rare case> catagory:\\n{xx.to_string()}\")\n",
        "# ----------------------------------------------------------------------- END (Feature enginearing)\n",
        "dtypes = DTYPES()\n",
        "# ---------------------------------------------------- Correlation plot\n",
        "new_line()\n",
        "cor_df = df.select_dtypes('number').corr().abs()\n",
        "mask = np.triu(np.ones_like(cor_df, dtype=bool));\n",
        "f, ax = plt.subplots(figsize=(17, 10));\n",
        "cmap = sns.color_palette(\"viridis\", as_cmap=True);\n",
        "plot_ = sns.heatmap(cor_df, mask=mask, cmap=cmap, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
        "plot_.axes.set_title(\"abs (Correlation) plot\",fontsize=25);\n",
        "plt.show()\n",
        "# ---------------------------------------------------------------------\n",
        "#===\n",
        "# m = 0\n",
        "for row in dtypes.iterrows():\n",
        "    # m += 1\n",
        "    # if m == 3:\n",
        "        # break\n",
        "    column_name, type_ = row[1]\n",
        "    x = df[column_name]\n",
        "    to_print = \"\\n\\n\\n========================================= {column_name} =========================================\\n\\n\"\n",
        "    print(colored(to_print, 'red'))\n",
        "\n",
        "    for col_ in df.columns:\n",
        "        if col_ == column_name:\n",
        "            continue\n",
        "        if df[col_].nunique() == df[column_name].nunique():\n",
        "            unique_combination = df[[col_, column_name]].drop_duplicates()\n",
        "            if unique_combination.apply(lambda x:x.is_unique).sum() == 2:\n",
        "                new_line()\n",
        "                to_print = f\"This Columns is duplicate of <{col_}> column\"\n",
        "                print(colored(to_print, 'red'))\n",
        "\n",
        "    # print(f\"Column Type     : {type_}\")\n",
        "    print(f\"Column Type     : \", end=\"\")\n",
        "    print(colored(type_, 'red'))\n",
        "    if x.isna().all():\n",
        "        new_line()\n",
        "        df.drop(columns=column_name, inplace=True)\n",
        "        print(colored(\"We dropped This column, because it is all Empty\", 'red'))\n",
        "        continue\n",
        "    if type_ in [\"O\", \"Date\"]:\n",
        "        if x.is_unique:\n",
        "            new_line()\n",
        "            df.drop(columns=column_name, inplace=True)\n",
        "            to_print = f\"We dropped This column, because it's a {type_} columns, and it's all values are unique\"\n",
        "            print(colored(to_print, 'red'))\n",
        "            continue\n",
        "    if x.nunique() == 1:\n",
        "        new_line()\n",
        "        df.drop(columns=column_name, inplace=True)\n",
        "        print(colored(\"We dropped This column, because There is only one unique value\", 'red'))\n",
        "        continue\n",
        "\n",
        "    if type_ == \"Number\":\n",
        "        local_cor = cor_df[column_name].drop(column_name).reset_index()\n",
        "        local_cor = local_cor.reindex(local_cor[column_name].abs().sort_values().index)\n",
        "        if local_cor[column_name].max() == 1:\n",
        "            new_line()\n",
        "            to_print = f\"This column is perfactly correlated with column <{local_cor[local_cor[column_name] == 1]['index'].values[0]}, so remove one of them\"\n",
        "            print(colored(to_print, 'red'))\n",
        "\n",
        "        new_line()\n",
        "        xm = local_cor[-3:].rename(columns={'index' : 'Column name', column_name : 'Correlation'}).reset_index(drop=True)\n",
        "        xm.index = xm['Column name']\n",
        "        xm.drop(columns=\"Column name\", inplace=True);\n",
        "        xm.plot(kind='barh', grid=True, figsize=(10,1.5));\n",
        "        plt.title(\"Most 3 correlated features with this columns (sorted)\", size=14);\n",
        "        plt.xlabel(\"Correlation\", size=12);\n",
        "        plt.show();\n",
        "\n",
        "        new_line()\n",
        "        skewness = x.skew(skipna = True)\n",
        "        if abs(skewness) < 0.5:\n",
        "            print(f\"The data is fairly symmetrical (skewness is: {skewness})\")\n",
        "        elif abs(skewness) < 1:\n",
        "            print(f\"The data are moderately skewed (skewness is: {skewness})\")\n",
        "        else:\n",
        "            to_print = f\"The data are highly skewed (skewness is: {skewness})\\nNote: When skewness exceed |1| we called it highly skewed\"\n",
        "            print(colored(to_print, 'red'))\n",
        "\n",
        "        # f = x.describe()\n",
        "        # f['Nunique'] = x.nunique()\n",
        "        # f['Nunique ratio'] = f.loc[\"Nunique\"] / f.loc[\"count\"] * 100\n",
        "        # f['Outlies count'] = (((x - x.mean())/x.std()).abs() > 3).sum()\n",
        "        # f['Outlies ratio'] = f.loc[\"Outlies count\"] / f.loc[\"count\"] * 100\n",
        "        # f['Nagative values count'] = (x < 0).sum()\n",
        "        # f['Nagative values ratio'] = f['Nagative values count'] / f['count'] * 100\n",
        "\n",
        "        ff = [x.count(), x.isna().sum(), x.mean(), x.std(), x.min()]\n",
        "        ff += x.quantile([.25,.5,.75]).to_list()\n",
        "        ff += [x.max(), x.nunique(), (((x - x.mean())/x.std()).abs() > 3).sum(), (x < 0).sum(), (x == 0).sum()]\n",
        "\n",
        "        f = pd.DataFrame(ff, index=['Count', 'NA', 'Mean', 'Std', 'Min', '25%', '50%', '75%', 'Max', 'Nunique', 'Outlies', 'Nagetive', 'Zeros'], columns=['Count'])\n",
        "        f['Ratio'] = f.Count / x.count() * 100\n",
        "        f.loc['Mean' : 'Max', 'Ratio'] = None\n",
        "\n",
        "        new_line()\n",
        "        print(f.round(2).to_string())\n",
        "        plot_numerical_columns(column_name)\n",
        "\n",
        "    elif type_ == \"Object\":\n",
        "        # f = x.describe()\n",
        "        # f = x.agg(['count', pd.Series.nunique])\n",
        "        # f['len'] = len(x)\n",
        "        # f['Na count'] = x.isna().sum()\n",
        "        # f['Na ratio'] = f['Na count'] / f['count'] * 100\n",
        "        # f['Most frequent'] = x.mode().values[0]\n",
        "        # f['Most frequent count'] = (x == f['Most frequent']).sum()\n",
        "        # f['Most frequent ratio'] = f['Most frequent count'] / f['count'] * 100\n",
        "        # f['Least frequent'] = x.value_counts().tail(1).index[0]\n",
        "        # f['Least frequent count'] = (x == f['Least frequent']).sum()\n",
        "        # f['Least frequent ratio'] = f['Least frequent count'] / f['count'] * 100\n",
        "        # f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        # f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "        l = x.count(), x.nunique(), len(x), x.isna().sum(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        f = pd.DataFrame(l, index=['Count', 'Nunique', 'Len', 'NA', 'Most frequent', 'Least frequent', 'Values occured only once'], columns=['Counts'])\n",
        "        f['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "        f.loc[['Len'], 'Ratio'] = None\n",
        "\n",
        "        new_line()\n",
        "        print(f.to_string())\n",
        "\n",
        "\n",
        "        if x.str.lower().nunique() != x.nunique():\n",
        "            new_line()\n",
        "            to_print = f\"Case issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin lower verstion there are   {x.str.lower().nunique()} unique values.\\n\"\n",
        "            print(colored(to_print, 'red'))\n",
        "\n",
        "        if x.str.strip().nunique() != x.nunique():\n",
        "            new_line()\n",
        "            to_print = f\"Space issue\\n\\tin orignal variable There are {x.nunique()} unique values\\n\\tin striped verstion there are {x.str.strip().nunique()} unique values.\"\n",
        "            print(colored(to_print, 'red'))\n",
        "\n",
        "        plot_catagorical_columns(column_name)\n",
        "\n",
        "    elif type == \"Date\":\n",
        "\n",
        "        new_line()\n",
        "        rd = relativedelta.relativedelta( pd.to_datetime(x.max()), pd.to_datetime(x.min()))\n",
        "        to_print = f\"Diffrenece between first and last date:\\n\\tYears : {rd.years}\\n\\tMonths: {rd.months}\\n\\tDays  : {rd.days}\"\n",
        "        print(colored(to_print, 'red'))\n",
        "\n",
        "        # f = pd.Series({'Count' : x.count(),\n",
        "        #             'Nunique count' : x.nunique(),\n",
        "        #             'Nunique ratio' : x.nunique() / x.count() * 100,\n",
        "        #             'Most frequent value' : str(x.mode()[0]),\n",
        "        #             'Least frequent value' :  x.value_counts().tail(1).index[0]\n",
        "        #             })\n",
        "        # f['Most frequent count'] = (x == f['Most frequent value']).sum()\n",
        "        # f['Most frequent ratio'] = f['Most frequent count'] / f['Count'] * 100\n",
        "        # f['Least frequent count'] = (x == f['Least frequent value']).sum()\n",
        "        # f['Least frequent ratio'] = f['Least frequent count'] / f['Count'] * 100\n",
        "        # f['Values occured only once count'] = x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        # f['Values occured only once Ratio'] = f['Values occured only once count'] / x.count() * 100\n",
        "\n",
        "        ff = x.count(), x.nunique(), (x == x.mode().values[0]).sum(), (x == x.value_counts().tail(1).index[0]).sum(), x.value_counts().where(lambda x:x==1).dropna().size\n",
        "        f = pd.DataFrame(ff, index=[\"Count\", 'Nunique', 'Most frequent values', 'Least frequent values', 'Values occured only once count'], columns=['Counts'])\n",
        "        f['Ratio'] = (f.Counts / x.count() * 100).round(4)\n",
        "\n",
        "        new_line()\n",
        "        print(f\"\\n{f.to_string()}\")\n",
        "\n",
        "\n",
        "        f = set(np.arange(x.dt.year.min(),x.dt.year.max()+1)).difference(\n",
        "            x.dt.year.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(colored(\"These Years (in order) are missing:\\n\", 'red'))\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "\n",
        "        f = set(np.arange(x.dt.month.min(),x.dt.month.max()+1)).difference(\n",
        "            x.dt.month.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(colored(\"These Months (in order) are missing:\\n\", 'red'))\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "\n",
        "        f = set(np.arange(x.dt.day.min(),x.dt.day.max()+1)).difference(\n",
        "            x.dt.day.unique())\n",
        "        if f:\n",
        "            new_line()\n",
        "            print(colored(\"These Days (in order) are missing:\\n\", 'red'))\n",
        "            for i in f:\n",
        "                print(\"\\t\", i, end=\", \")\n",
        "\n",
        "        new_line()\n",
        "        plot_date_columns(column_name)\n",
        "\n",
        "\n",
        "# ================================================================================================================ Modeling\n",
        "print(\"\\n\\n\")\n",
        "print(\"----------------------------------------------------------------------------------------------\")\n",
        "print(\"****************************************** Modeling ******************************************\")\n",
        "\n",
        "# Regression problem\n",
        "if df[target_variable].dtype in [float, int]:\n",
        "\n",
        "    print(\"\\n-------------------- This is Regression problem --------------------\\n\")\n",
        "    print(\"''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\")\n",
        "\n",
        "    df_T = df.select_dtypes(\"number\")\n",
        "    cat_cols = pd.get_dummies(df.select_dtypes(exclude=\"number\"), prefix_sep=\"__\")\n",
        "    df_T[cat_cols.columns.to_list()] = cat_cols\n",
        "\n",
        "    df = df_T.copy(\"deep\")\n",
        "    del df_T\n",
        "    del cat_cols\n",
        "    # ====\n",
        "    train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "    # ====\n",
        "    # --------------------------------------------------------- Linear regression\n",
        "    to_print = \"\\n ------------------------------------- Linear Regression -------------------------------------\\n\"\n",
        "    print(colored(to_print, 'red'))\n",
        "\n",
        "    model_reg = OLS(train_y, train_X).fit()\n",
        "    summary = model_reg.summary()\n",
        "    summary_df = pd.DataFrame(summary.tables[1])\n",
        "    summary_df.columns = summary_df.iloc[0]\n",
        "    summary_df.drop(0, inplace=True)\n",
        "    summary_df.columns = summary_df.columns.astype(str)\n",
        "    summary_df.columns = [\"Variable\"] + summary_df.columns[1:].to_list()\n",
        "    for i in summary_df.columns[1:]:\n",
        "        summary_df[i] = summary_df[i].astype(str).astype(float)\n",
        "    summary_df.Variable = summary_df.Variable.astype(str)\n",
        "    summary_df['Indicator'] = summary_df['P>|t|'].apply(lambda x:\"***\" if x < 0.001 else \"**\" if x < 0.01 else \"*\" if x < 0.05 else \".\" if x < 0.1  else \"\")\n",
        "    summary_df = summary_df.sort_values(\"Variable\").reset_index(drop=True)\n",
        "    summary_df.to_csv()\n",
        "    new_line()\n",
        "    print(colored(\"NOTE: This summary saved as <summary_OLS_1.csv>\", 'red'))\n",
        "\n",
        "    new_line()\n",
        "    print(summary_df.to_string())\n",
        "    # ============================= Model statistic\n",
        "    predictions = model_reg.predict(test_X)\n",
        "\n",
        "    new_line()\n",
        "    print(colored(\" --- Model statistic --- \\n\", 'red'))\n",
        "    print(f\"R-squared         : {round(model_reg.rsquared, 3)}\")\n",
        "    print(f\"Adj. R-squared    : {round(model_reg.rsquared_adj, 3)}\")\n",
        "    print(f\"F-statistic       : {round(model_reg.fvalue)}\")\n",
        "    print(f\"Prob (F-statistic): {model_reg.f_pvalue}\")\n",
        "    print(f\"No. Observations  : {round(model_reg.nobs)}\")\n",
        "    print(f\"AIC               : {round(model_reg.aic)}\")\n",
        "    print(f\"Df Residuals      : {round(model_reg.df_resid)}\")\n",
        "    print(f\"BIC               : {round(model_reg.bic)}\")\n",
        "    print(f\"RMSE (test)       : {RMSE(predictions)}\")\n",
        "    # ======\n",
        "    f = train_X.copy(\"deep\")\n",
        "    f['Errors__'] = model_reg.resid\n",
        "    f = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "    new_line()\n",
        "    print(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "    print(f\"Mean of train reseduals: {model_reg.resid.mean()}\")\n",
        "    del f\n",
        "    # ============================= END (Model statistic)\n",
        "    # --------------------------------------------------------- END Linear regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------- Random Forest\n",
        "    print(\"\\n ------------------------------------- Random Forest -------------------------------------\\n\")\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators = 200, oob_score=True)\n",
        "    model_rf = rf.fit(train_X, train_y);\n",
        "    predictions_rf = rf.predict(test_X)\n",
        "\n",
        "    new_line()\n",
        "    print(colored(\"RF model peramters:\\n\", 'red'))\n",
        "    pprint.pprint(model_rf.get_params())\n",
        "\n",
        "    new_line()\n",
        "    importances = list(rf.feature_importances_)\n",
        "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(test_X, importances)]\n",
        "    featuresImportance = pd.Series(model_rf.feature_importances_, index=train_X.columns).sort_values(ascending=False)\n",
        "    if len(featuresImportance) > 30:\n",
        "        featuresImportance = featuresImportance.head(30)\n",
        "    featuresImportance.plot(figsize=(20,10), kind='bar', grid=True);\n",
        "    plt.title(\"RandomForest Feature importances Graph\", size=18,color='red');\n",
        "    plt.xlabel(\"Features\", size=14, color='red');\n",
        "    plt.ylabel(\"Importance\", size=14, color='red');\n",
        "    plt.show();\n",
        "    del featuresImportance\n",
        "\n",
        "    new_line()\n",
        "    print(colored(\"--- Model statistic ---\", 'red'))\n",
        "    # The coefficient of determination R^2 of the prediction.\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "    print(f\"R^2 (test) : {rf.score(test_X, test_y)}\")\n",
        "    print(f\"R^2 (train): {rf.score(train_X, train_y)}\")\n",
        "    print(f\"RMSE (test): {RMSE(predictions_rf)}\")\n",
        "    print(f\"oob score  : {model_rf.oob_score_}\")\n",
        "\n",
        "    f = test_X.copy(\"deep\")\n",
        "    errors_rf = predictions_rf - test_y\n",
        "    f['Errors__'] = errors_rf\n",
        "    f = f.corr()['Errors__'].drop(\"Errors__\").abs().sort_values().dropna().tail(1)\n",
        "    new_line()\n",
        "    print(f\"Maximum correlation between Reseduals and any data columns is {f.values[0]}, with columns <{f.index[0]}>\")\n",
        "    # --------------------------------------------------------- END Random Forest\n",
        "elif df[target_variable].dtype == \"O\":\n",
        "    # Classififcation problem\n",
        "    if df[target_variable].nunique() == 2:\n",
        "        print(\"\\n-------------------- This is Binary Classification problem --------------------\\n\")\n",
        "        print(\"''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\")\n",
        "        df = pd.concat([\n",
        "                        df.select_dtypes(exclude = \"O\"),\n",
        "                        pd.get_dummies(df.drop(columns=target_variable).select_dtypes(\"O\")),\n",
        "                        df[[target_variable]]\n",
        "                        ], 1)\n",
        "\n",
        "        train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "        clf = LogisticRegression().fit(train_X, train_y)\n",
        "        predictions = clf.predict_proba(test_X)\n",
        "        predictions = pd.Series(predictions[:, 0])\n",
        "        lst = []\n",
        "        for thresh in np.linspace(predictions.min(), predictions.max(), 50)[1:]:\n",
        "            pred = predictions < thresh\n",
        "\n",
        "            pred.loc[pred == True] = clf.classes_[0]\n",
        "            pred.loc[pred == False] = clf.classes_[1]\n",
        "\n",
        "            test_y = test_y.reset_index(drop=True)\n",
        "\n",
        "            TN = ((pred == clf.classes_[0]) & (test_y == clf.classes_[0])).sum()\n",
        "            TP = ((pred == clf.classes_[1]) & (test_y == clf.classes_[1])).sum()\n",
        "            FN = ((pred == clf.classes_[0]) & (test_y == clf.classes_[1])).sum()\n",
        "            FP = ((pred == clf.classes_[1]) & (test_y == clf.classes_[0])).sum()\n",
        "\n",
        "            p = TP / (TP + FP)\n",
        "            r = TP / (TP + FN)\n",
        "            f = 2 * ((p * r) / (p+r))\n",
        "\n",
        "            lst.append((thresh, (pred == test_y).mean(), p, r , f))\n",
        "\n",
        "        d = pd.DataFrame(lst, columns=[\"Thresold\", \"Accurecy(0-1)\", \"Precision\", \"Recall\", \"F1\"])\n",
        "        d = d.set_index(\"Thresold\")\n",
        "        d.plot(grid=True, figsize=(18,7));\n",
        "        plt.title(\"Model performance at diffrent Thresolds\", size=18, color='red');\n",
        "        plt.xlabel(\"Thresold\", size=14, color='red');\n",
        "        plt.ylabel(\"\");\n",
        "        plt.show()\n",
        "    else:\n",
        "        to_print = \"\\n-------------------- This is Multiclass Classification problem --------------------\\n\"\n",
        "        print(colored(to_print, 'red'))\n",
        "        print(\"'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\")\n",
        "\n",
        "        df.loc[:, df.select_dtypes(\"O\").columns] = df.select_dtypes(\"O\").apply(lambda x: pd.Series(LabelEncoder().fit_transform(x.astype(str))).astype(str))\n",
        "        train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target_variable), df[target_variable])\n",
        "\n",
        "        clf=RandomForestClassifier(n_estimators=1000).fit(train_X, train_y)\n",
        "        predictions = clf.predict(test_X)\n",
        "        feature_imp = pd.Series(clf.feature_importances_,index=train_X.columns).sort_values(ascending=False)\n",
        "        if feature_imp.size > 30:\n",
        "            feature_imp = feature_imp.head(30)\n",
        "        feature_imp.plot(kind='barh', figsize=(17,10), grid=True);\n",
        "        plt.title(\"Feature importances Graph\", size=18, color='red');\n",
        "        plt.xlabel(\"Importance\", size=14, color='red');\n",
        "        plt.ylabel(\"Feature\", size=14, color='red');\n",
        "        plt.show()\n",
        "        # ====\n",
        "        f = (test_y, predictions)\n",
        "        f_int = (test_y.astype(int), predictions.astype(int))\n",
        "\n",
        "        print(f\"accuracy_score: {metrics.accuracy_score(*f)}\")\n",
        "        print(f\"f1_score: {metrics.f1_score(*f_int)}\")\n",
        "\n",
        "        metrics.plot_roc_curve(clf, test_X, test_y);\n",
        "        plt.title(\"ROC curve plot\");\n",
        "        plt.show();\n",
        "\n",
        "        metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(*f)); plt.show()\n",
        "\n",
        "        metrics.plot_confusion_matrix(clf, test_X, test_y);\n",
        "        plt.title(\"Confusion matrix\");\n",
        "        plt.show()\n",
        "\n",
        "        metrics.plot_precision_recall_curve(clf, test_X, test_y);\n",
        "        plt.title(\"Precision recall curve\");\n",
        "        plt.show()\n",
        "# ================================================================================================================ END Modeling\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}