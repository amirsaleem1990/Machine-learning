The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.

Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.

While using information Gain as a criterion, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous.

trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.

the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to over fit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting.

While using information Gain as a algorithm, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous.

There is a high probability of overfitting in Decision Tree.

Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.

Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.

Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.

Records are distributed recursively on the basis of attribute values.

Decision trees require relatively little effort from users for data preparation.

Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.

Non-Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.

The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions.

The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.
-----------------------
Select the best attribute using Attribute Selection Measures(ASM) to split the records.
Make that attribute a decision node and breaks the dataset into smaller subsets.
Starts tree building by repeating this process recursively for each child until one of the condition will match:
    All the tuples belong to the same attribute value.
    There are no more remaining attributes.
    There are no more instances.
ASM (Attribute Selection Measures) provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute
-----------------------
assumptions:
	Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model. 
	Records are distributed recursively on the basis of attribute values. 
We have different attributes selection measures to identify the attribute which can be considered as the root note at each level.
	Entropy,
	Information gain,
	Gini index,
	Gain Ratio,
	Reduction in Variance
	Chi-Square


Entropy:
	Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information.


The algorithm selection is also based on the type of target variables. Let us look at some algorithms used in Decision Trees:

ID3 → (extension of D3)
	ID3 follows the rule — A branch with an entropy of zero is a leaf node and A brach with entropy more than zero needs further splitting.
C4.5 → (successor of ID3)
CART → (Classification And Regression Tree)
CHAID → (Chi-square automatic interaction detection Performs multi-level splits when computing classification trees)
MARS → (multivariate adaptive regression splines)

-------------------------------
Q: How C4.5 split continuous attribute?
A: For continuous attribute, the algorithm will always try to split it into 2 branches only.

Suppose we have a training set with an attribute “age” which contains following values.
    Age : 10, 11, 16, 18, 20, 35

Now at a node, the algorithm will consider following possible splitting
    Age <=10 & Age>10
    Age <=11 & Age>11
    Age <=16 & Age>16
    Age <=18 & Age>18
    Age <=20 & Age>20

You can see that if there are N possible values, we would have to consider N-1 possible splits.
And note that we do not choose the mid-point between values as the splitting threshold. (We won’t consider Age <=10.5 as 10.5 never appears in the training set)
------------------------------
In case of attributes with continuous values, ID3, C4.5 are not effective. FUZZY ID3 is an effective algorithm to employ. I have used it in my project to classify and predict the operating point of IEEE 30-bus system. As we know in case of power system with attributes such as voltage, current, active power, reactive power, power angle, … we have purely continuous attributes where C4.5 would fail. btw fuzzzy ID3 was effective.
------------------------------
Lets say you have two continuous variables - Age and Income. Age has values from 10 - 80 and income has 20,000 to 90,000.

Now the decision tree check the split for first variable Age at 20, 30, 40, 50…, 80. If 40 gives the most pure split, second best is by 50 and third best is 80 and suppose the worst was at 20. The decision tree would not go and check 21, 22, 23, 24,… etc. But it will definitely check 41, 42, 43…

Effectively we are checking the split at every number but in a more smart manner. The same goes with the income variable.
------------------------------
When finding the entropy for a splitting decision in a decision tree, you find a threshold (such as midpoint or anything you come up with), and count the amount of each class label on each size of the threshold. For example:

Var1 | Class
0.75 | 1
0.87 | 0
0.89 | 1
0.96 | 0
1.02 | 1
1.05 | 1
1.14 | 1
1.25 | 1

Let's assume a threshold of 0.99 because I want two equal-sized bins and that's the midpoint between 0.96 and 1.02. The classes on the left are [1, 0, 1, 0] (since it's half/half, entropy is 1.0), and on the right [1, 1, 1, 1] (since it's all equal, entropy is 0.0).

To make a table like the one you show here, you need to define N threshold points (2.0, 3.5, ..., 7.5, in your case), which will produce N+1 bins of data, you take the class of each bin and calculate the entropy.
-----------------------------
