The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.

Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.

While using information Gain as a criterion, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous.

trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.

the splitting process results in fully grown trees until the stopping criteria is reached. But, the fully grown tree is likely to over fit data, leading to poor accuracy on unseen data. This bring ‘pruning’. Pruning is one of the technique used tackle overfitting.

While using information Gain as a algorithm, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous.

There is a high probability of overfitting in Decision Tree.

Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.

Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.

Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.

Records are distributed recursively on the basis of attribute values.

Decision trees require relatively little effort from users for data preparation.

Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.

Non-Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.

The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions.

The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.
-----------------------
Select the best attribute using Attribute Selection Measures(ASM) to split the records.
Make that attribute a decision node and breaks the dataset into smaller subsets.
Starts tree building by repeating this process recursively for each child until one of the condition will match:
    All the tuples belong to the same attribute value.
    There are no more remaining attributes.
    There are no more instances.
ASM (Attribute Selection Measures) provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute
-----------------------
assumptions:
	Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model. 
	Records are distributed recursively on the basis of attribute values. 
We have different attributes selection measures to identify the attribute which can be considered as the root note at each level.
	Entropy,
	Information gain,
	Gini index,
	Gain Ratio,
	Reduction in Variance
	Chi-Square


Entropy:
	Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information.

	