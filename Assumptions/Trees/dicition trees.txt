While using information Gain as a algorithm, we assume attributes to be categorical, and for gini index, attributes are assumed to be continuous.

There is a high probability of overfitting in Decision Tree.

Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.

Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.

Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.

Records are distributed recursively on the basis of attribute values.

Decision trees require relatively little effort from users for data preparation.

Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.

Non-Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.

