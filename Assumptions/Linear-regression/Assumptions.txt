Correlated means that changes in one variable is parallel to a change in another variable. 

Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results. Luckily, decision trees and boosted trees algorithms are immune to multicollinearity by nature. When they decide to split, the tree will choose only one of the perfectly correlated features. However, other algorithms like Logistic Regression or Linear Regression are not immune to that problem and you should fix it before training the model.


Linear relationship
	The linearity assumption can best be tested with scatter plots
Multivariate normality
	the linear regression analysis requires all variables to be multivariate normal.  This assumption can best be checked with a histogram or a Q-Q-Plot.  Normality can be checked with a goodness of fit test, e.g., the Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation (e.g., log-transformation) might fix this issue.
No or little multicollinearity:
	Numerical stability aside, prediction given by OLS model should not be affected by multicolinearity, as overall effect of predictor variables is not hurt by presence of multicolinearity. It is interpretation of effect of individual predictor variables that are not reliable when multicolinearity is present.
A note about sample size.  In Linear regression the sample size rule of thumb is that the regression analysis requires at least 20 cases per independent variable in the analysis.


for Rasiduals:

	1- Homoscedasticity (have a constant variance)
		If the residuals fan out as the predicted values increase, then we have what is known as heteroscedasticity. This means that the variability in the response is changing as the predicted value increases.
		A scatter plot of residual values vs predicted values is a goodway to check for homoscedasticity.There should be no clear pattern in the distribution and if there is a specific pattern,the data is heteroscedastic.

	2- be approximately normally distributed (with a mean of zero), and
	3- No autocorrelation (be independent of one another).
	4- The X variables and residuals are uncorrelated 
	If the assumptions are met, the residuals will be randomly scattered around the center line of zero, with no obvious pattern. The residuals will look like an unstructured cloud of points, centered at zero. If there is a non-random pattern, the nature of the pattern can pinpoint potential issues with the model.



Q: Why removing highly correlated features is important?
A: (Assuming you are talking about supervised learning)
Correlated features will not always worsen your model, but they will not always improve it either.
There are three main reasons why you would remove correlated features:
    1- Make the learning algorithm faster
Due to the curse of dimensionality, less features usually mean high improvement in terms of speed.
If speed is not an issue, perhaps don't remove these features right away (see next point)
    2- Decrease harmful bias
The keyword being harmful. If you have correlated features but they are also correlated to the target, you want to keep them. You can view features as hints to make a good guess, if you have two hints that are essentially the same, but they are good hints, it may be wise to keep them.

Some algorithms like Naive Bayes actually directly benefit from "positive" correlated features. And others like random forest may indirectly benefit from them.

Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other, and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a "good" feature, whereas if you remove B for instance, this chance drops to 1/2

Of course, if the features that are correlated are not super informative in the first place, the algorithm may not suffer much.

So moral of the story, removing these features might be necessary due to speed, but remember that you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection embedded in them.

A good way to deal with this is to use a wrapper method for feature selection. It will remove redundant features only if they do not contribute directly to the performance. If they are useful like in naive bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting)

    3- Interpretability of your model
If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's razor. If your model is not "that much" worse with less features, then you should probably use less features.

----------------------
Correlated features mean you can reduce overfitting (through column sampling) without giving up too much predictive quality.
----------------------
In the context of machine learning we usually use PCA to reduce the dimension of input patterns. This approach considers removing correlated features by someway (using SVD) and is an unsupervised approach. This is done to achieve the following purposes:

    Compression
    Speeding up learning algorithms
    Visualizing data
    Dealing with curse of high dimensionality

Although this may not seem okay but I have seen people that use removing correlated features in order to avoid overfitting but I don't think it is a good practice. For more information I highly recommend you to see here.
----------------------
The answer to this question depends greatly upon the purpose of the model. In inference, highly correlated features are a well-known problem. For example, two features highly correlated with each other and with y, might both come out as insignificant in an inference model, potentially missing an important explanatory signal. Therefore, in inference it is generally recommended to thin them out.

If your supervised learning is for prediction, the answer - counter to conventional wisdom - is usually the opposite. The only reason to remove highly correlated features is storage and speed concerns. Other than that, what matters about features is whether they contribute to prediction, and whether their data quality is sufficient.

Noise-dominated features will tend to be less correlated with other features, than features correlated with y. Hence, as mentioned above in the example by Valentin, thinning out the latter will increase the proportion of the former.

In particular, methods like random forests and KNN treat all features equally, so thinning out correlated features directly reduces their signal-to-noise ratio.

Methods that auto-select features like single trees, "pure" lasso, or neural networks, might be less affected. But even then, other than longer computing time, there is rarely anything to lose prediction-wise from keeping correlated features in the mix.
----------------------
A key goal of regression analysis is to isolate the relationship between each independent variable and the dependent variable. The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant. That last portion is crucial for our further discussion about multicollinearity.

The idea is that you can change the value of one independent variable and not the others. However, when independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable. The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison.
What Problems Do Multicollinearity Cause?

Multicollinearity causes the following two basic types of problems:

    The coefficient estimates can swing wildly based on which other
    independent variables are in the model. The coefficients become very sensitive to small changes in the model.
    Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.

Imagine you fit a regression model and the coefficient values, and even the signs, change dramatically depending on the specific variables that you include in the model. It’s a disconcerting feeling when slightly different models lead to very different conclusions. You don’t feel like you know the actual effect of each variable!

Now, throw in the fact that you can’t necessarily trust the p-values to select the independent variables to include in the model. This problem makes it difficult both to specify the correct model and to justify the model if many of your p-values are not statistically significant.

As the severity of the multicollinearity increases so do these problematic effects. However, these issues affect only those independent variables that are correlated. You can have a model with severe multicollinearity and yet some variables in the model can be completely unaffected.
Do I Have to Fix Multicollinearity?

Multicollinearity makes it hard to interpret your coefficients, and it reduces the power of your model to identify independent variables that are statistically significant. These are definitely serious problems. However, the good news is that you don’t always have to find a way to fix multicollinearity!

The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:

    The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.
    Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables. If high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the experimental variables without problems.
    Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity. (Reference: "The fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations." — Applied Linear Statistical Models, p289, 4th Edition.)
----------------------
When variables as super highly correlated, they can introduce instability in the errors on the betas and are, in fact, measuring the same thing, roughly speaking. 
----------------------
co-variance is not dimensionless and it depends on units. For example of you use centimetres instead of meters, your co-variance values will be higher. Correlation is normalised and dimensionless.
----------------------
the answer is no, you cannot always remove correlated features. Because combinations of those features may be meaningful for your task, even if each of them are not very useful and they are correlated. Still, the possibility of information gain from correlated features is not ignored usually and various feature selection or dimensionality reduction techniques (such as PCA) are employed, rather than simply discarding features based on correlation.
----------------------
Correlated features in naive bayes can kill accuracy and may have very little effect in a random forest.
----------------------
First, if you have enough data, you could try to assess how harmful it is to keep both variables. For instance, with a training set and a test set, it wil help you see if you over fit.

Second, you may have to keep both variables because users of the analysis will need you to. It’s very hard to accept at first, but there are times where you have to keep a variable in a model because, else, people won’t use the model at all, even if the other well correlated variable is better. So you may try to keep both and assess as said before how harmfull it is to keep both.
----------------------
High covariance doesn’t have to mean there is strong correlation
----------------------
There are some techniques that allow you to transform your data into new features that aren’t correlated. Like Principal Component Analysis (that creates new “variables” that aren’t correlated) .

You can also use Pearson correlation coefficient to remove highly correlated features from the dataset. However if you model has the capacity to model non-linear relationships between the data maybe you are better off with some non-linear correlation coefficient. There are some like: Maximal Correlation or the less known (but not worst) Distance correlation coefficient.
----------------------
For a logistic regression, highly correlated features might will result in highly unstable parameter estimates.

say you have 4 features A,B,C,and D.  Say you are trying to predict the class of X.
Say, you have a multicollinearity between A, B, and C. Something like:
A+B = C
When you take one sample to find the parameter estimates, the estimate of C can get distributed between A and B. For some other sample it might get distributed between A and C and so on an so forth. This is one basic way to understand multi-collinearity problem in logistic regression. Given that, stable parameter estimates is one of the important ways to judge logistic regression, multi-collinearity does affect the performance.
----------------------
you do not want to have highly correlated features because their parameter estimates will be unstable. That is, they could be far off the mark, and/or inappropriate when applied out of sample.
----------------------
So in order to remove highly correlated features you can:

    1- Use PCA to reduce dimensionality, or,
    2- Use decision tree to find the important features, or,
    3- You may manually choose features from your knowledge (if it is possible) which features are more promising to help you to classify your data, or,
    4- You can combine some features to a new feature manually such that saying one feature may eliminate the necessity to tell another set of features as those are likely can be inferred from that single feature.
----------------------
PCA is used to remove multicollinearity from the data. If there are correlated variables, then PCA replaces them with a principle component which can explain max variance.
----------------------
But severe multicollinearity is a major problem, because it increases the variance of the regression coefficients, making them unstable. The more variance they have, the more difficult it is to interpret the coefficients.
----------------------
how do you know if you need to be concerned about multicollinearity in your regression model? Here are some things to watch for:

    A regression coefficient is not significant even though, theoretically, that variable should be highly correlated with Y.
    When you add or delete an X variable, the regression coefficients change dramatically.
    You see a negative regression coefficient when your response should increase along with X.
    You see a positive regression coefficient when the response should decrease as X increases.
    Your X variables have high pairwise correlations. 

----------------------
One way to measure multicollinearity is the variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated.  If no factors are correlated, the VIFs will all be 1, but if the VIF is greater than 1, the predictors may be moderately correlated. A VIF between 5 and 10 indicates high correlation that may be problematic. And if the VIF goes above 10, you can assume that the regression coefficients are poorly estimated due to multicollinearity.
----------------------
If multicollinearity is a problem in your model -- if the VIF for a factor is near or above 5 -- the solution may be relatively simple. Try one of these: 

    1- Remove highly correlated predictors from the model.  If you have two or more factors with a high VIF, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared.  Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value. 
     
    2- Use Partial Least Squares Regression (PLS) or Principal Components Analysis, regression methods that cut the number of predictors to a smaller set of uncorrelated components.


----------------------
Multicollinearity causes problem in regression. If two or more predictor variables in a multiple regression model are highly correlated, the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data.  With and without one of them, the coefficients for some correlated factor may change sign, may change magnitude; the standard error will increase a lot. Say x1, x2 and x3 are correlated with a linear relationship.  Without x3, the model fit well, both x1 and x2 are significant, adding x3 in the model, the model fitting will not get improve, the standard error for the regression coefficient will be much higher, the regression coefficients for x1 and x2 will change a lot.
There are usually two aims by using linear regression: identification of the predictor effect and prediction. For prediction purpose, multicoloinerity has no “bad” effect on it. You don’t need use PCA or ridge to make corrections. But for identification, the multicollinearity must be considered, hence you can use PCA or ridge or other approach (say Lasso, as someone suggested).
There are several ways to handle multi-collinearity in the regression.  Principal component analysis and ridge regression are popular used. Those approached are included in most popular Statistical software.
Using PCA: if there is an eigenvalue is near 0, then the eigenvector give a function relationship among the vectors. You may do transformation according the function, or do the analysis by using eigenvectors.  When write the factor as linear combination of the eigenvectors, avoid the one,  the combination with large coefficients on the eigenvector with near zero eigenvalue. That one has larger standard error.
Ridge regression estimates tend to be stable in the sense that they are usually little affected by small changes in the data on which the fitted regression is based. In contrast, ordinary least squares estimates may be highly unstable under these conditions when the independent variables are highly collinearity.
A major limitation of ridge regression is that ordinary inference procedures are not applicable and exact distributional properties are not known. Another limitation is that the choice of the biasing constant k is a judgmental one. While formal methods have been developed foo making this choice, these methods have their own limitations.
The common used method for selection of the constant of k: assume you have p independent variables (predictors)
Select k from 0 to certain giving value, say as follows
         Ridge (k) = (0.00 0.001 to .009 by .002 0.010 to 0.03 by 0.002 0.03 to 0.09 by 0.01 0.1 to 1.0 by 0.1 1 to 10 by 0.5)
List all estimation of the predictors in a table
Or draw a graph contain p curves: regression coefficients VS k,
For those predictors with collinerity , you can see the big changes of regression coefficients  across k
Select the k such that all of regression coefficient get stable
Using iteration
Initial k=0  
K1=p*regression variance (k=0)/sum of  the regression coefficients (k=0)^2  for all p predictors)
K_(n+1)= p*regression variance (k=n)/sum of  the regression coefficients (k=n)^2  for all p predictors)
When k_n, K_(n+1)  similar , stop.
----------------------
When you are dealing with multicollinearity, it means that at least two predictors are highly correlated together. If VIF values are above 5, then multicollinearity may pose a problem, above 10 it is "disaster".
To deal with this you may remove one predictor at a time, check if VIF values are acceptable (between 1 and 5, above 10 you are having serious problems) and also observe how the explanation of the variance in the DV is affected (it is good to see if r2 is reduced after any predictor is removed). 
Alternatively, you may apply Partial Least Square Regression of Principal Components Analysis, but the first solution should do. 
----------------------
    When one independent variable is perfectly correlated with another independent variable (or with a combination of two or more other independent variables), a unique least-squares solution for regression coefficients does not exist.
    When one independent variable is highly correlated with another independent variable (or with a combination of two or more other independent variables), the marginal contribution of that independent variable is influenced by other independent variables. As a result:
        Estimates for regression coefficients can be unreliable.
        Tests of significance for regression coefficients can be misleading.
----------------------
Note: Multicollinearity makes it hard to assess the relative importance of independent variables, but it does not affect the usefulness of the regression equation for prediction. Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem. 
----------------------
There are two popular ways to measure multicollinearity:
	1- compute a coefficient of multiple determination(adjusted R2) for each independent variable, or 
		If we ignore the dependent variable, we can compute a coefficient of multiple determination (R2k) for each of the k independent variables. We do this by regressing the kth independent variable on all of the other independent variables. That is, we treat Xk as the dependent variable and use the other independent variables to predict Xk.

		How do we interpret R2k? If R2k equals zero, variable k is not correlated with any other independent variable; and multicollinearity is not a problem for variable k. As a rule of thumb, most analysts feel that multicollinearity is a potential problem when R2k is greater than 0.75; and, a serious problem when R2k is greater than 0.9. 

	2- compute a variance inflation factor for each independent variable.
		The variance inflation factor is another way to express exactly the same information found in the coefficient of multiple correlation. A variance inflation factor is computed for each independent variable, using the following formula:

		VIFk = 1 / ( 1 - R2k )

		where VIFk is the variance inflation factor for variable k, and R2k is the coefficient of multiple determination for variable k. 

		The interpretation of the variance inflation factor mirrors the interpretation of the coefficient of multiple determination. If VIFk = 1, variable k is not correlated with any other independent variable. As a rule of thumb, multicollinearity is a potential problem when VIFk is greater than 4; and, a serious problem when it is greater than 10. The output above shows a VIF of 2.466, which indicates some multicollinearity but not enough to worry about.

Bottom line: If R2k is greater than 0.9 or VIFk is greater than 10, it is likely that regression coefficients are poorly estimated. And significance tests on those coefficients may be misleading. 

If you only want to predict the value of a dependent variable, you may not have to worry about multicollinearity. Multiple regression can produce a regression equation that will work for you, even when independent variables are highly correlated.

How to Deal with Multicollinearity:
	Increase sample size. Other things being equal, a bigger sample means reduced sampling error. The increased precision may overcome potential problems from multicollinearity. 

	Remove one or more of the highly-correlated independent variables. Then, define a new regression equation, based on the remaining variables. Because the removed variables were redundant, the new equation should be nearly as predictive as the old equation; and coefficients should be easier to interpret because multicolinearity is reduced.

	Define a new variable equal to a linear combination of the highly-correlated variables. Then, define a new regression equation, using the new variable in place of the old highly-correlated variables.

Note: Multicollinearity only affects variables that are highly correlated. If the variable you are interested in has a small R2j, statistical analysis of its regression coefficient will be reliable and informative. That analysis will be valid, even when other variables exhibit high multicollinearity. 
-----------------------------
In general, multicollinearity can lead to wider confidence intervals and less reliable probability values for the independent variables. That is, the statistical inferences from a model with multicollinearity may not be dependable.
-----------------------------
we can safely say that a multicollinearity or collinearity will not affect the results of predictions from decision trees. During inference from the decision tree models though, it is important to take how each feature may be affected by another into account to help make valuable business decisions.
-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

-----------------------------

